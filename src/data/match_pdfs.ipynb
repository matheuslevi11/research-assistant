{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6de8618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from pyzotero import zotero\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ZOTERO_USER_ID = os.getenv('ZOTERO_USER_ID')\n",
    "ZOTERO_API_KEY = os.getenv('ZOTERO_API_KEY')\n",
    "\n",
    "zot = zotero.Zotero(ZOTERO_USER_ID, 'user', ZOTERO_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "313e948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "library_items = zot.everything(zot.collection_items_top('SVUM4G2M', limit=106))\n",
    "library = pd.read_csv('zotero_pdf_matches.csv', encoding='windows-1252')\n",
    "\n",
    "for item in library_items:\n",
    "    title = item['data']['title'].replace('â€™', \"'\").replace('â€“', '-')\n",
    "    matched_rows = library[library['title'] == title]\n",
    "    if not matched_rows.empty:\n",
    "        pdf_name = matched_rows.iloc[0]['pdf_name']\n",
    "        with open(f\"zotero_cache_metadata/{pdf_name.replace('.pdf', '')}.json\", \"w\") as f:\n",
    "            json.dump(item, f, indent=4)\n",
    "    else:\n",
    "        print(f\"Title: {title} -> PDF Name: Not Found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd88c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nContents lists available at ScienceDirect\\nInformation Fusion\\njournal homepage: www.elsevier.com/locate/inffus\\n \\nFull length article\\nFaceExpr: Personalized facial expression generation via attention-focused \\nU-Net feature fusion in diffusion models\\nMuhammad Sher Afgan a\\n, Bin Liu a\\n,âˆ—, Mamoona Naveed Asghar b, Wajahat Khalid a\\n, \\nKai Zou a, Dianmo Sheng a\\na School of Cyber Science and Technology, University of Science and Technology of China, 96 Jinzhai Road, Hefei, 230026, Anhui, China\\nb School of Computer Science, University of Galway, University Road, Galway, H91 TK33, Galway, Ireland\\nA R T I C L E  I N F O\\nDataset link: https://github.com/MSAfganUST\\nC/FaceExpr.git\\nKeywords:\\nAttention\\nDiffusion\\nExpressions synthesis\\nFusion\\nIdentity preservation\\nText-to-image\\n \\nA B S T R A C T\\nText-to-image diffusion models have revolutionized image generation by creating high-quality visuals from text \\ndescriptions. Despite their potential for personalized text-to-image applications, existing standalone methods \\nhave struggled to provide effective semantic modifications, while approaches relying on external embeddings \\nare computationally complex and often compromise identity and face fidelity. To overcome these challenges, \\nwe propose FaceExpr, an innovative three-instance framework using standalone text-to-image models that \\nprovide accurate facial semantic modifications and synthesize facial images with diverse expressions, all \\nwhile preserving the subjectâ€™s identity. Specifically, we introduce a person-specific fine-tuning approach with \\ntwo key components: (1) Attention-Focused Fusion, which uses an attention mechanism to align identity \\nand expression features by focusing on critical facial landmarks, preserving the subjectâ€™s identity, and (2) \\nExpression Text Embeddings, integrated into the U-Net denoising module to resolve language ambiguities \\nand enhance expression accuracy. Additionally, an expression crafting loss is employed to strengthen the \\nalignment between identity and expression. Furthermore, by leveraging the prior preservation loss, we enable \\nthe synthesis of expressive faces in diverse scenes, views, and conditions. FaceExpr establishes state-of-the-art \\nperformance over both standalone and hybrid methods, demonstrating its effectiveness in controllable facial \\nexpression generation. It shows strong potential for personalized content generation in digital storytelling, \\nimmersive virtual environments, and advanced research applications. For code please visit: https://github.\\ncom/MSAfganUSTC/FaceExpr.git\\n1. Introduction\\nThe rapid growth in deep learning research for face recognition \\nand emotion analysis has underscored the critical need for diverse \\nand comprehensive datasets to train robust and high-performing mod-\\nels [1â€“5]. However, as privacy concerns over real-world facial data \\nincrease, the priority has shifted to finding alternative approaches. \\nSynthetic face generation has emerged as a cutting-edge data augmen-\\ntation technique [6,7], creating large, diverse datasets. These synthetic \\ndatasets provide the development of robust facial images without re-\\nlying on real-world sensitive data [8]. Despite these advancements, \\nthe challenge of obtaining high-quality annotated data that accurately \\nrepresents a wide range of ethnicities and diverse expressions re-\\nmains [9]. This limitation prevents synthetic datasets from fully solving \\nthe problem.\\nGenerative modeling has significantly advanced data synthesis, \\nstarting with variational autoencoders (VAE) [10,11], followed by \\nâˆ—Corresponding author.\\nE-mail address: flowice@ustc.edu.cn (B. Liu).\\ngenerative adversarial networks (GANs) revolutionized data augmenta-\\ntion [12â€“15]. GANs excelled in generating large datasets while ensuring \\nprivacy [16,17], offering fine-grained control over attributes like pose \\nand expression [18â€“20]. However, challenges remain in providing \\ncontrol over multi-domain data (e.g., text, audio, video), emphasizing \\nthe need for further innovation.\\nDiffusion models [21â€“23] have emerged as a superior alternative to \\nGANs, demonstrating remarkable flexibility in diverse generation tasks. \\nThey excel in uni-modal image-to-image applications, including image \\nediting [24] and inpainting [25], as well as in cross-modal tasks such \\nas high-quality text-to-image [26,27], audio-to-image [28], audio-to-\\nvideo [29], and image-to-text applications like image-to-caption gen-\\neration [30]. Building on this versatility, diffusion models enable ad-\\nvanced multi-modal applications, particularly subject-driven content \\nhttps://doi.org/10.1016/j.inffus.2025.103431\\nReceived 19 February 2025; Received in revised form 11 May 2025; Accepted 15 June 2025\\nInformation Fusion 125 (2026) 103431 \\nAvailable online 4 July 2025 \\n1566-2535/Â© 2025 Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies. \\nM.S. Afgan et al.\\nsynthesis [31]. Further expanding on these advances, diffusion mod-\\nels offer significant potential for personalized text-to-image genera-\\ntion [32â€“34], where a set of input images is used to perform user-\\ndefined modifications, achieving enhanced customization in image syn-\\nthesis.\\nDespite advances in personalized text-to-image generation, the chal-\\nlenge of modifying facial expressions without compromising identity, \\nremains largely underexplored. Existing standalone diffusion-based per-\\nsonalization methods [31,32,34] prioritize subject identity preserva-\\ntion but lack facial expression modification. While some approaches \\nhave explored semantic control in personalized generation, they ad-\\ndress distinct challenges rather than directly targeting facial expression \\ngeneration. For instance, [35] proposes a general disentanglement \\nframework without specializing in expressions; [36] improves temporal \\nconsistency but lacks semantic flexibility; [37] emphasizes identity \\npreservation via linear embeddings, and [38] mitigates identity drift \\nthrough gradient-based fine-tuning without targeting facial expressions. \\nNone of these approaches explicitly tackle facial expression generation.\\nIn response to the limitations of standalone approaches, hybrid \\nmethods, [39â€“42] have emerged, combining pre-trained text-to-image \\ndiffusion models with external modules, such as face/expression recog-\\nnition models and custom-trained embedding extractors, to enable \\nprecise manipulation of facial attributes. While representing the cur-\\nrent state-of-the-art in expression generation, these hybrid techniques \\nintroduce several critical constraints that limit their practical adop-\\ntion. First, they require large-scale datasets for fine-tuning to align \\nthe external modules with the pre-trained text-to-image model. Sec-\\nond, their integration incurs significant computational overhead due \\nto joint fine-tuning of auxiliary and base models. More importantly, \\nthese hybrid approaches often suffer from feature misalignment, where \\nexternal embeddings conflict with the domain of the pretrained text-to-\\nimage model due to the different training objectives. This misalignment \\ndegrades output quality, either through artifacts or by compromising \\nthe preservation of identity.\\nIn response to the challenges outlined above and motivated by the \\nlimitations in existing methods, we have identified and pursued the \\nfollowing research questions in this paper:\\nâ€¢ RQ1: How can a diffusion-based framework be developed for \\naccurate facial expression generation that avoids external em-\\nbeddings and overcomes challenges like large datasets, feature \\nmisalignment, and high computational cost?\\nâ€¢ RQ2: How can semantic attributes be effectively modified using \\na standalone diffusion-based models to synthesize accurate facial \\nexpressions, while maintaining identity?\\nâ€¢ RQ3: How can the connection between expressive fused iden-\\ntity and expression prompt be effectively established to reduce \\nlanguage ambiguities, enhance expression intensity, and how can \\nfeature alignment be further strengthened?\\nTaking these research questions into consideration, the key contribu-\\ntions of our work are:\\nâ€¢ We propose FaceExpr, an innovative identity-specific fine-tune, \\nthree-instance framework based solely on standalone diffusion \\nmodels, without external embeddings. It synthesizes expressive \\nfacial images of identity on a textual prompt across diverse en-\\nvironments (addressing RQ1).\\nâ€¢ To achieve precise alignment of identity and expression features, \\nwe introduce a novel intermediate feature fusion technique us-\\ning an attention mechanism within the U-Net denoising module \\n(addressing RQ2).\\nâ€¢ To mitigate potential language discrepancies and enhance ex-\\npression intensity, we introduce expression text prompt em-\\nbeddings within the U-Net denoising module. Furthermore, we \\nemploy expression crafting loss to strengthen the alignment of the \\nfeatures (addressing RQ3).\\nâ€¢ Our method achieves optimized performance with a focus on \\ncomputational efficiency, identity preservation, and expression \\naccuracy, offering a balance between complexity and resource \\nutilization.\\nIn summary, FaceExpr is the first innovative three-instance frame-\\nwork that achieves accurate and semantically rich facial expression \\nsynthesis built on standalone diffusion models, eliminating reliance on \\nexternal components like GANs, VAEs, or facial expression recognition \\nmodules. The framework leverages three instances of a pretrained \\ntext-to-image diffusion model: one for expression guidance via feature \\nextraction, a second for feature fusion through a novel attention mecha-\\nnism, and a third to leverage the capabilities of pretrained text-to-image \\nmodels by using prior preservation loss. The proposed FaceExpr frame-\\nwork enables controllable synthesis of high-fidelity, identity-preserving \\nexpressive faces with diverse facial attributes, accessories, actions, and \\nbackground scenes without the need for large-scale datasets or exten-\\nsive fine-tuning, thereby significantly reducing computational time (for \\nan overview of FaceExprâ€™s synthesis capabilities, see Fig. 1).\\nThe paper is organized as follows: Section 2 provides an overview \\nof related work. Section 3 discusses the preliminaries of denoising \\ndiffusion models and text-to-image generation. Section 4 presents the \\nproposed approach. Section 5 covers implementation details, experi-\\nmental setup, and results. Section 6 compares the proposed approach \\nwith both standalone and hybrid models, using performance metrics \\nand insights from user feedback studies. Furthermore, ablation studies \\nare performed to evaluate the influence of each component and in-\\nstance. Section 7 summarizes the work and suggests avenues for further \\nresearch.\\n2. Related work\\nThis section reviews recent developments in personalized text-to-\\nimage generation and facial expression synthesis, highlighting key \\nlimitations that motivate our method. We categorize prior work into \\ntwo broad paradigms: (1) standalone methods, which rely solely on \\ndiffusion-based approach, and (2) hybrid methods, which incorporate \\nexternal modules or embeddings. Our method follows the standalone \\nparadigm, utilizing end-to-end diffusion framework without auxiliary \\nnetworks or external guidance.\\nWe analyze existing methods across three dimensions: (1) the core \\ntechnique, (2) architectural design (e.g., single, dual, or multi-instance), \\nand (3) explicit expression control. A comparative summary is provided \\nin Table 1. This structured comparison offers an in-depth analysis \\nof each approach and provides a clear basis for distinguishing our \\nframework, both architecturally and functionally, from prior methods.\\n2.1. Personalized text-to-image generation\\nPersonalized text-to-image generation aims to synthesize images \\nthat retain the subjectâ€™s identity while adapting to user-defined contex-\\ntual settings, such as background, scenes, and lighting conditions [32â€“\\n38,43â€“45]. A key objective in this field is maintaining optimal identity \\npreservation while avoiding unintended alterations to irrelevant details \\nthat could compromise the subjectâ€™s identity. Many existing approaches \\ntightly couple identity with semantic attributes, which limits the ability \\nto modify specific attributes such as facial expressions without causing \\nidentity drift [46].\\nSeveral techniques have been developed to achieve effective person-\\nalization using pre-trained text-to-image models. The seminal work of \\nTextual Inversion [31] introduced pseudo-tokens for concept learning \\nbut suffered from limited identity fidelity due to their low-dimensional \\nrepresentations. DreamBooth [32] addressed this through full model \\nfine-tuning, linking the subjectâ€™s identity to a unique identifier and \\nembedding it into latent space. However, these approaches primarily \\nfocus on the provided subjectâ€™s identity preservation and do not allow \\nInformation Fusion 125 (2026) 103431 \\n2 \\nM.S. Afgan et al.\\nFig. 1. FaceExpr, a novel standalone diffusion model framework, is capable of generating photorealistic personalized facial expression images with diverse accessories, global \\nattributes, actions, and contexts while preserving both identity and expression accuracy with high fidelity.\\nTable 1\\nComparison of design and technique with existing standalone, and hybrid methods across key dimensions. \\nFaceExpr uniquely follows the standalone paradigm and integrates (1) no external embeddings, (2) no \\ndisentanglement, (3) three-instance specialization, (4) attention-based fusion, and (5) explicit expression \\ncontrol.\\n \\nMethod\\nApproach\\nDisent.\\nInstance\\nExpr.-control  \\n \\nstandalone\\nTextual Inversion [31]\\nLearns pseudo-words for full concept\\nNo\\nSingle\\nÃ—\\n \\n \\nDreamBooth [32]\\nCouples identity with identifier tokens\\nNo\\nDual\\nÃ—\\n \\n \\nDisenBooth [33]\\nDisentangles identity and attributes\\nYes\\nDual\\nÃ—\\n \\n \\nCeleb-Basis [37]\\nLinear basis for identity\\nNo\\nSingle\\nÃ—\\n \\n \\nCIP-TIG [38]\\nCross-initialization\\nNo\\nSingle\\nÃ—\\n \\n \\nhybrid\\nEmojiDiff [40]\\nSelf E-Adapter\\nYes\\nDual\\nâœ“\\n \\n \\nDiffSFSR [39]\\nArcFace+FaceNet+DLib\\nNo\\nSingle\\nâœ“\\n \\n \\nW+ Adapter [41]\\nStyleGAN\\nNo\\nSingle\\nâœ“\\n \\n \\nMasterWeaver [42]\\nCLIP-Image\\nNo\\nSingle\\nâœ“\\n \\n \\nFaceExpr (Ours)\\nAttention-based fusion\\nNo\\nThree\\nâœ“\\n \\nfor semantic modifications due to the tightly coupled identity with \\nsemantic attributes, limiting flexibility in modifying facial expressions.\\nMore recent efforts, such as DisenBooth [33], proposed explicit \\ndisentanglement losses to separate identity from attributes. However, \\nthey fall short in specializing this disentanglement toward expression \\ncontrol, often treating expressions as a subset of broader attribute ma-\\nnipulation. Similarly, CelebBasis [37] uses linear identity embeddings \\nfor efficient identity transfer, but struggles with semantic flexibility \\nand expression-specific variation due to its reliance on single-image \\nencoding. CIP-TIG [38] tackled the tension between identity preser-\\nvation and attribute modification using a cross-initialization scheme \\nduring fine-tuning. While effective in mitigating identity degradation, \\nthis approach still treats expression generation as incidental, not a \\nprimary goal.\\nUnlike the above standalone methods, our proposed approach ex-\\nplicitly targets facial expression generation in personalized settings, \\na challenge often neglected or indirectly addressed in prior work. \\nWe introduce two core innovations that distinguish our method from \\nprevious standalone frameworks: a three-instance architecture (Sec-\\ntion 4), an attention-based feature fusion mechanism within the U-Net \\n(Section 4.2.1). These contributions allow us to achieve a more nuanced \\nbalance between identity preservation and expression. Our method, \\nFaceExpr, generates expressive variations of a subject while preserving \\ncritical identity details. A comparative summary is provided in Table 1, \\nwhich outlines the design and approach of each method, highlighting \\nthe distinguishing features of our framework.\\n2.2. Face expression generation\\nFacial expression generation using GANs has shown promising re-\\nsults [47â€“50]. However, these methods often lack efficient textual \\ncontrol, limiting their flexibility and adaptability. In contrast, diffusion \\nmodels have emerged as a powerful alternative, enabling text-guided \\nimage generation [51] and offering greater versatility. This capability \\nopens up new possibilities for the controlled generation of expressive \\nfaces through textual input.\\nRecently, hybrid approaches have emerged that combine pre-trained \\ntext-to-image models with additional pre-trained learned embeddings \\nfor facial expression generation. For instance, DiffSFSR [39] utilizes \\nrecognition models like ArcFace [52], FaceNet [53], and DLib [54]\\nto extract identity-related embeddings, and DLN [55] to extract\\nexpression-related embeddings. It then uses a multi-layer perceptron \\nnetwork to provide conditional control to the pre-trained text-to-\\nimage model. Similarly, EmojiDiff [40] trains an additional refiner \\nfrom scratch to guide expressions using same-identity triplet data, \\nenabling expression transfer. The W + adapter [41] extends the latent \\nspace of StyleGAN to leverage embeddings for expression guidance. \\nMasterWeaver [42] employs a CLIP image encoder to extract reference \\nface features, aligning the editing direction of expression control with \\nthe text-to-image model.\\nHowever, relying on external pre-trained models presents significant \\nchallenges: (1) high computational costs for embedding extraction from \\nexternal modules, (2) the need for embedding pre-processing to ensure \\nInformation Fusion 125 (2026) 103431 \\n3 \\nM.S. Afgan et al.\\narchitectural alignment, (3) the requirement for large-scale datasets \\nto enable intensive fine-tuning for multi-objective optimization, and \\n(4) prolonged training times for feature alignment [56], as the base \\nmodel was not originally optimized for these external embeddings. \\nMoreover, such embeddings often further degrade output fidelity, as \\ntheir latent space is misaligned with the text-to-image modelâ€™s native \\nrepresentation.\\nIn contrast, FaceExpr offers a more efficient and streamlined solu-\\ntion by eliminating the need for external embeddings, as highlighted \\nin Table 1. By introducing a self-contained diffusion-based framework \\nwhere feature fusion occurs internally, FaceExpr addresses all key chal-\\nlenges associated with external embeddings. Furthermore, by operating \\nwithin the native representation space of the diffusion model, it avoids \\nfidelity degradation caused by latent space mismatches. This design not \\nonly enhances computational efficiency but also ensures the generation \\nof accurate, identity-preserving, and expressive facial images.\\n3. Preliminaries\\nIn this section, we provide a brief overview of the foundational \\nknowledge discussed in this paper. Section 3.1 introduces the denois-\\ning diffusion models. Section 3.2 presents the U-Net architecture of \\ntext-to-image models.\\n3.1. Denoising diffusion model\\nDiffusion models are inspired by systematically adding progres-\\nsive noise, utilizing Markov Chain state transitions [57], and drawing \\ninsights from non-equilibrium statistical physics [58]. This approach \\nestablishes a robust and adaptable framework for comprehensively \\nanalyzing distributions. Moreover, it leverages the intrinsic connection \\nbetween diffusion probabilistic models and denoising score matching \\nvia Langevin Dynamics [59] to be utilized for data synthesis [21,22]. \\nThese models operate by introducing a forward and reverse process, \\nwith the forward process progressively adding Gaussian noise, while \\nthe reverse process denoises the data.\\nIn more detail, diffusion models are latent models characterized by \\nconditional probability densities [21]. The latent representation ğ‘§0 âˆ¼\\nğ‘(ğ‘§0) corresponds to the data, and ğ‘¡ represents the time steps in the \\nforward and reverse processes. In the forward process, Gaussian noise \\nwith mean \\nâˆš\\n1 âˆ’ğ›½ğ‘¡ğ‘§ğ‘¡âˆ’1 and variance ğ›½ğ‘¡ğ¼ is added to the data, with \\nğ›½ğ‘¡ being either learned or set as a constant [10]. The reverse process \\nutilizes a learnable mean ğğœƒ for denoising, and KL divergence is used to \\ncompare Gaussians to optimize the model. This design ultimately aims \\nto minimize the negative log-likelihood, facilitating the generation of \\nhigh-quality samples [21].\\nThe objective function is: \\nğ¿DM(ğœƒ) âˆ¶= Eğ‘¡,ğ³0,ğâˆ¼\\ue23a(ğŸ,ğˆ)\\n[\\nâ€–â€–â€–ğœ–âˆ’ğœ–ğœƒ\\n(ğ³ğ‘¡, ğ‘¡)â€–â€–â€–\\n2]\\n,\\n(1)\\nwhere ğ¿DM(ğœƒ) is the MSE between the actual noise ğœ– and the predicted \\nnoise ğœ–ğœƒ(ğ³ğ‘¡, ğ‘¡) at time step ğ‘¡, which the model minimizes.\\nFurthermore, diffusion models have been extended to incorporate \\nexternal conditions such as text [60], segmentation masks [61], or \\n3D mappings [62], which guide the diffusion process to synthesize \\nconditional data. The Stable Diffusion model [26] is one of the state-\\nof-the-art (SOTA) models that generate high-quality visual images from \\ntext descriptions.\\n3.2. U-Net architecture of text-to-image\\nThe architecture of Stable Diffusion consists of an encoder, a de-\\ncoder, and an additional denoising U-Net module. These components \\ncollaborate to produce detailed and accurate image generations condi-\\ntioned on textual prompts. The U-Net module operates through down-\\nsampling and upsampling stages, which are integral to the diffusion \\nprocess. The stages of the diffusion U-Net can be described as follows: \\nğ‘‹ğ‘ƒ\\nğ‘¡âˆˆ[ğ‘‡,0] = ğ‘ˆNet-Eğœ™(â†“(ğ‘…Net\\n(ğ¶atn(ğ‘‹), ğ¸ğ‘ƒ\\nemb\\n) , ğ‘¡emb\\n))\\n(2)\\nğ‘Œğ‘ƒ\\nğ‘¡âˆˆ[ğ‘‡,0] = ğ‘ˆNet-Dğœ™(â†‘(ğ‘…Net\\n(ğ¶atn(ğ‘‹ğ‘¡), ğ¸ğ‘ƒ\\nemb\\n) , ğ‘¡emb\\n))\\n(3)\\nwhere ğ‘‹ğ‘ƒ\\nğ‘¡âˆˆ[ğ‘‡,0] represents the downsampled features while denoising at \\ntime step ğ‘¡ of the encoder ğ¸ğœ™ in the U-Net module [63]. Each step \\ninvolves four downsampling operations, preceded by the embedding of \\nan external conditional prompt ğ¸ğ‘ƒ\\nğ‘’ğ‘šğ‘ integrated through cross-attention \\nğ¶atn and a Residual Network ğ‘…Net block. Subsequently, the decoder \\nğ·ğœ™ applies the same structure in an upsampling manner, with ğ‘Œğ‘ƒ\\nğ‘¡âˆˆ[ğ‘‡,0]\\nrepresenting the upsampled features. Our approach builds upon this by \\nextracting expression-specific semantic features from the U-Net residual \\nblock during denoising. These features are fused with additional prompt \\nembeddings to fine-tune a second instance of the model.\\n4. Proposed framework\\nAs illustrated in Fig. 2, we introduce FaceExpr, an innovative \\nthree-instance framework specifically designed to tackle the task of \\nidentity-preserving facial expression generation in diverse, user-defined \\ncontextual settings, all driven solely by textual conditions. FaceExpr is \\ndesigned to coordinate three dedicated instances of pretrained text-to-\\nimage diffusion models, each assigned to a distinct and complementary \\nsubtask.\\nFramework Workflow: Instance 1: Expression Guider generates \\nexpressive faces conditioned on an expression prompt ğ‘ƒ(exp), extract-\\ning intermediate spatial features that guide the subsequent instance, \\nas detailed in Section 4.1.1. Instance 2: Personalized Expression \\nGenerator (PEG) is the cornerstone of our framework, fine-tune the \\nmodel instance using input identity images paired with a text prompt \\nğ‘ƒ(id), which contains a unique identifier [xyz*] and the subjectâ€™s class \\n(e.g., â€˜â€˜man/womanâ€™â€™). Specifically, by incorporating our novel feature \\nfusion mechanism, which dynamically aligns expression features (from \\nInstance 1) with identity features, as described in Section 4.2.1, and\\nAdditional Prompt Embedding, which integrates expression-specific \\nembeddings, enabling a connection with the expressive, fused iden-\\ntity as outlined in Section 4.2.2. Additionally, it employs triple-loss \\noptimization, which jointly optimizes identity preservation, expression \\ncrafting loss to strengthen the feature alignment, and prior preserva-\\ntion loss to retain the prior knowledge. Instance 3: Context Guider\\nprovides the prior knowledge needed to integrate context using the text \\nprompt ğ‘ƒ(context), addressing the limitation of catastrophic forgetting \\nduring fine-tuning as discussed in Section 4.3.\\n4.1. Instance 1: Expression guidance\\nThis sub-section outlines the methodology for expression guidance \\nin our framework. Section 4.1.1 provides details on features extraction \\nrequired for fusion.\\n4.1.1. Expression feature extraction\\nIn Instance 1 of our approach, we extract features from the 2 and \\n3 downsampling blocks of the U-Net while generating an expression-\\nconditioned image guided by the prompt ğ‘ƒ(ğ‘’ğ‘¥ğ‘) (e.g., â€˜â€˜A photo of a \\nmanâ€™s face, expression*â€™â€™, where expression represents expressions like \\nhappy or sad), as illustrated in Fig. 3. These blocks progressively cap-\\nture higher-level patterns by aggregating low-level visual features [63,\\n64], and their convolutional layers focus on key dynamic facial re-\\ngions, such as the eyes, mouth, and eyebrows, which are essential for \\nconveying expressions.\\nSimultaneously, these layers minimize the influence of static\\nidentity-specific features, allowing the model to prioritize non-identity \\nInformation Fusion 125 (2026) 103431 \\n4 \\nM.S. Afgan et al.\\nFig. 2. The proposed FaceExpr fine-tuning framework utilizes 3 instances of pre-trained text-to-image diffusion models: Instance 1: Expression Guider generates expressive facial \\nimages using prompt ğ‘ƒ(ğ‘’ğ‘¥ğ‘), extracting relevant features for guidance. Instance 2: Personalized Expression Generator (PEG) fine-tunes the model using input identity images and \\nprompt ğ‘ƒ(ğ‘–ğ‘‘), incorporating expression features through attention-focused feature fusion along with an expression prompt. Meanwhile, identity preservation loss is applied to retain \\nthe identity, and expression crafting loss to strengthen the feature alignment. In parallel, Instance 3: Context Guider applies prior preservation loss to leverage the semantic \\nprior, ensuring diverse instance generation and seamless context integration.\\nFig. 3. Illustration of attention-focused fusion and integrating expression prompt embeddings. The intermediate expression features from Instance 1, ğ‘“ğ‘›\\nexp, along with the identity \\nfeatures from Instance 2, ğ‘“ğ‘›\\nid, are processed separately for each block n through the mutual attention. This results in the fused features ğ‘“ğ‘›\\nfus, corresponding to Blocks, respectively. \\nThe fused features for each block are then integrated independently during the upsampling phase of Instance 2. Additional prompt embedding ğ‘ƒ(exp) is added to the identity \\nprompt ğ‘ƒ(id) during fine-tuning on a set of identity images ğ¼(ğ‘¥)\\nid .\\nattributes while reducing identity-related details [65,66]. These fea-\\ntures are computed using Eq. (4). \\nğ‘“ğ‘›\\nexp = ğ‘‹\\nğ‘ƒ(ğ‘’ğ‘¥ğ‘)\\nğ‘¡âˆˆ[0.3ğ‘‡,0.7ğ‘‡]\\n(4)\\nğ‘“ğ‘›\\nexp represents the expression features, with ğ‘› indicating the block \\nnumber. ğ‘‹ refers to the downsampling process, ğ‘ƒ(ğ‘’ğ‘¥ğ‘) as the expression \\nprompt, while ğ‘¡ denotes the time step range. Fig. 4 (Instance 1: Expres-\\nsion Guider) illustrates the feature representations at the downsampling \\nblocks, focusing on blocks 2 and 3 during the time steps from 0.3ğ‘‡ to \\n0.7ğ‘‡ for expression extraction.\\nAlgorithm 1 is designed to extract facial expression features by \\nprocessing the noisy input image ğ¼, the expression-guiding prompt \\nğ‘ƒ(ğ‘’ğ‘¥ğ‘), and the number of time steps ğ‘‡steps. The outer loop iterates from \\nğ‘‡steps to 0, capturing temporal features using time step embedding ğ‘‡emb. \\nThe inner loop applies downsampling for specific time steps (from 0.7ğ‘‡\\nto 0.3ğ‘‡) using a U-Net architecture. The upsampling features ğ‘Œğ‘‡ are \\nprocessed for reconstruction, and output the expressive face image as \\nğ¼exp, alongside the extracted features ğ‘“ğ‘›\\nexp.\\n4.2. Instance 2: Personalized expression generator (PEG)\\nThis sub-section outlines the methodology for the instance 2 of the \\nframework. Section 4.2.1 covers attention-focused feature fusion, and \\nSection 4.2.2 discusses additional expression prompt embedding.\\n4.2.1. Attention-focused feature fusion\\nIn Instance 2 (PEG), we extract the downsampled identity features, \\nsimilar to the above Section 4.1.1, denoted as ğ‘“ğ‘›\\nid, where ğ‘› denotes the \\nblock number, extracted during denoising at time steps 0.3ğ‘‡ to 0.7ğ‘‡, \\nas shown in Fig. 3. Instead of fine-tuning with the noisy image, we \\nprovide ğ¼ğ‘¥\\nid (the identity facial images) along with the identity-specific \\nconditional prompt ğ‘ƒ(ğ‘–ğ‘‘), which includes an identity token (e.g., â€˜â€˜a \\nInformation Fusion 125 (2026) 103431 \\n5 \\nM.S. Afgan et al.\\nFig. 4. Visualization of downsampling features from instance 1 and 2 across various time steps. The features highlighted in red correspond to blocks 2 and 3, extracted during \\nthe time-step range from 0.7ğ‘‡ to 0.3ğ‘‡. These features are employed for the attention-based spatial feature fusion.\\nAlgorithm 1 Expression Feature Extraction\\nInputs: ğ¼, ğ‘ƒ(ğ‘’ğ‘¥ğ‘), ğ‘‡ğ‘ ğ‘¡ğ‘’ğ‘ğ‘ \\nfor ğ‘¡= ğ‘‡ğ‘ ğ‘¡ğ‘’ğ‘ğ‘  to 0 do\\n \\nğ‘‡ğ‘’ğ‘šğ‘â† Emb(ğ‘‡ğ‘ ğ‘¡ğ‘’ğ‘ğ‘ )\\n \\nfor ğ‘–= 1 to 4 do\\n \\nif (ğ‘›= 2 or ğ‘›= 3) then\\n \\nif 0.3ğ‘‡ğ‘ ğ‘¡ğ‘’ğ‘ğ‘ â‰¤ğ‘¡â‰¤0.7ğ‘‡ğ‘ ğ‘¡ğ‘’ğ‘ğ‘  then\\n \\nğ‘“ğ‘›\\nexp â†ğ‘ˆNet-Eğœ™(â†“(ğ‘…Net(ğ‘ƒatten(ğ¼), ğ‘ƒ(ğ‘’ğ‘¥ğ‘)), ğ‘‡emb))\\n \\nend if\\n \\nğ‘‹â†ğ‘ˆNet-Eğœ™(â†‘(ğ‘…Net(ğ¶atten(ğ¼), ğ‘ƒ(ğ‘’ğ‘¥ğ‘)), ğ‘‡emb))\\n \\nelse\\n \\nğ‘‹â†ğ‘ˆNet-Eğœ™(â†‘(ğ‘…Net(ğ¶atten(ğ¼), ğ‘ƒ(ğ‘’ğ‘¥ğ‘)), ğ‘‡emb))\\n \\nend if\\n \\nend for\\n \\nfor ğ‘–= 1 to 4 do\\n \\nğ‘Œâ†ğ‘ˆNet-Dğœ™(â†‘(ğ‘…Net(ğ¶atten(ğ‘‹), ğ‘ƒ(ğ‘’ğ‘¥ğ‘)), ğ‘‡emb))\\n \\nend for\\nend for\\nReturn: (ğ¼ref, ğ‘“ğ‘›\\nexp)\\nphoto of [xyz*] manâ€™â€™), where [ğ‘¥ğ‘¦ğ‘§âˆ—] denotes the individualâ€™s unique \\nidentity. Fig. 4 (Instance 2: PEG) illustrates the feature maps from U-\\nNet downsampling blocks, emphasizing their role in capturing spatial \\ninformation from the identity during the time steps from 0.3ğ‘‡ to 0.7ğ‘‡.\\nThe features are extracted using Eq. (2) and are formulated as \\nfollows: \\nğ‘“ğ‘›\\nid = ğ‘‹\\nğ‘ƒ(ğ‘–ğ‘‘)\\nğ‘¡âˆˆ[0.3ğ‘‡,0.7ğ‘‡]\\n(5)\\nğ‘“ğ‘›\\nid represents the identity features, with ğ‘› indicating the block number, \\nğ‘‹ denotes the downsampling process, with ğ‘ƒ(ğ‘–ğ‘‘) being the prompt.\\nThe features from Instance 1 as ğ‘“ğ‘›\\nexp and 2 as ğ‘“ğ‘›\\nid have identical \\ndimensions: (640 Ã— 32 Ã— 32) for block 2 and (1280 Ã— 64 Ã— 64) for block \\n3. These features are fused using mutual cross-attention, where ğ‘“exp\\nserves as the query for the first attention, and ğ‘“id serves as both the \\nkey and value. Conversely, for the second attention, ğ‘“id serves as the \\nquery, while ğ‘“exp is used as both the key and value. The outputs of both \\nattention mechanisms are then added together. The fusion is given as \\nfollows: \\nğ‘“ğ‘›\\nfus = softmax\\n(ğ‘“ğ‘›\\nexp(ğ‘“ğ‘›\\nid)ğ‘‡\\nâˆš\\nğ‘‘ğ‘˜\\n)\\nğ‘“ğ‘›\\nid + softmax\\n(ğ‘“ğ‘›\\nid(ğ‘“ğ‘›\\nexp)ğ‘‡\\nâˆš\\nğ‘‘ğ‘˜\\n)\\nğ‘“ğ‘›\\nexp\\n(6)\\nwhere ğ‘“exp represents the expression features, while ğ‘“id represents the \\nidentity features. The variable ğ‘› refers to the block number (either ğ‘›= 2\\nor ğ‘›= 3, depending on the block). The fusion mechanism enables the \\nmodel to learn complex relationships between identity and expression. \\nRather than treating these features separately, the model adapts the \\nidentity features based on the desired expression. Additionally, the \\nattention mechanism processes each block individually. The approach \\nfocuses on preserving identity-related information by retaining only \\nthe essential spatial features for expression alignment while discarding \\nirrelevant features [66]. Fig. 5 illustrates the fused feature map ğ‘“fus\\nafter applying the attention.\\nAlgorithm 2 Attention-focused fusion\\nInputs: ğ¼ğ‘¥\\nId, ğ‘ƒ(ğ‘–ğ‘‘), ğ‘ƒ(ğ‘’ğ‘¥ğ‘), ğ‘‡ğ‘ ğ‘¡ğ‘’ğ‘ğ‘ , ğ‘“ğ‘›\\nexp\\nfor ğ‘¡= ğ‘‡ğ‘ ğ‘¡ğ‘’ğ‘ğ‘  to 1 do\\n \\nğ‘‡emb â† Emb(ğ‘‡steps)\\n \\nfor ğ‘–= 1 to 4 do\\n \\nfor ğ‘–= 1 to 4 do\\n \\nif (ğ‘›= 2 or ğ‘›= 3) then\\n \\nif 0.3ğ‘‡ğ‘ ğ‘¡ğ‘’ğ‘ğ‘ â‰¤ğ‘¡â‰¤0.7ğ‘‡ğ‘ ğ‘¡ğ‘’ğ‘ğ‘  then\\n \\nğ‘“ğ‘›\\nid â†ğ‘ˆNet-Eğœ™(â†“(ğ‘…Net(ğ‘ƒatten(ğ¼ğ‘–ğ‘‘), (ğ‘ƒ(ğ‘–ğ‘‘) + ğ‘ƒ(ğ‘’ğ‘¥ğ‘))), ğ‘‡emb))\\n \\nend if\\n \\nğ‘‹â†ğ‘ˆNet-Eğœ™(â†‘(ğ‘…Net(ğ¶atten(ğ¼ğ‘–ğ‘‘), ğ‘ƒ(ğ‘–ğ‘‘)), ğ‘‡emb))\\n \\nelse\\n \\nğ‘‹â†ğ‘ˆNet-Eğœ™(â†‘(ğ‘…Net(ğ¶atten(ğ¼ğ‘–ğ‘‘), ğ‘ƒ(ğ‘–ğ‘‘)), ğ‘‡emb))\\n \\nend if\\n \\nend for\\n \\nend for\\n \\nfor ğ‘–= 1 to 4 do\\n \\nif (ğ‘›= 2 or ğ‘›= 3) then\\n \\nif 0.3ğ‘‡ğ‘ ğ‘¡ğ‘’ğ‘ğ‘ â‰¤ğ‘¡â‰¤0.7ğ‘‡ğ‘ ğ‘¡ğ‘’ğ‘ğ‘  then\\n \\nğ‘“ğ‘›\\nfus = softmax\\n(\\nğ‘“ğ‘›\\nexp(ğ‘“ğ‘›\\nid)ğ‘‡\\nâˆš\\nğ‘‘ğ‘˜\\n)\\nğ‘“ğ‘›\\nid + ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥\\n(\\nğ‘“ğ‘›\\nid(ğ‘“ğ‘›\\nexp)ğ‘‡\\nâˆš\\nğ‘‘ğ‘˜\\n)\\nğ‘“ğ‘›\\nexp\\n \\nğ‘‹ğ‘›\\nskip â†ğ‘…Net(ğ‘Œâˆ¥ğ‘“ğ‘›\\nfus)\\n \\nğ‘Œğ‘›â†ğ‘ˆNet-Dğœ™(â†‘(ğ‘…Net(ğ¶atten(ğ‘‹ğ‘›\\nskip), (ğ‘ƒ(ğ‘–ğ‘‘) + ğ‘ƒ(ğ‘’ğ‘¥ğ‘))), ğ‘‡emb))\\n \\nend if\\n \\nXskip â†ğ‘…Net(ğ‘Œâˆ¥X)\\n \\nY â†ğ‘ˆNet-Dğœ™(â†‘(ğ‘…Net(ğ¶atten(Xskip), ğ‘ƒ(ğ‘–ğ‘‘)), ğ‘‡emb))\\n \\nend if\\n \\nend for\\nend for\\nReturn:(ğ¼gen)\\nThese fused features are integrated into the corresponding upsam-\\npling block of the U-Net denoising module in Instance 2, denoted as ğ‘“ğ‘›\\nfus\\nin Fig. 3. The detail is present in Algorithm 2, takes inputs the set of \\nInformation Fusion 125 (2026) 103431 \\n6 \\nM.S. Afgan et al.\\nFig. 5. Visualization of the feature map blocks 2 and 3, demonstrating the effect of attention fusion on spatial features: specifically for blocks 2 and 3 of instance 1 (ğ‘“exp) and \\ninstance 2 (ğ‘“id), emphasizing the alignment of expression spatial features with identity features.\\nidentity image ğ¼ğ‘¥\\nId, the prompts ğ‘ƒid and ğ‘ƒexp, time steps ğ‘‡steps, and the \\nexpression features ğ‘“ğ‘›\\nexp. A time embedding ğ‘‡emb is computed at each \\ntime step. During the middle time range (from 0.7ğ‘‡steps to 0.3ğ‘‡steps), \\nthe identity features ğ‘“id are extracted and fused with ğ‘“exp using the \\nattention mechanism, yielding the fused features ğ‘“fus. These features \\nare then passed into the corresponding upsampling blocks of the U-\\nNet via skip connections, denoted as ğ‘‹skip, ensuring proper alignment. \\nFinally, the model generates the output image ğ¼gen, which incorporates \\nboth the identity and the desired facial expression.\\n4.2.2. Expression prompt embedding\\nIn the above section, our attention-focused fusion mechanism com-\\nbines identity and expression features, resulting in a new expressive \\nfused identity. To establish a direct association between the fused ex-\\npressive identity and the expression token, we combine both prompts at \\ncorresponding time steps. Specifically, the first prompt, ğ‘ƒ(ğ‘–ğ‘‘), contains \\nan identity token, ensuring the generated face retains the subjectâ€™s \\nidentity. The second prompt, ğ‘ƒ(ğ‘’ğ‘¥ğ‘), specifies the desired expression \\ntoken. The combination is as follows: \\nğ‘ƒ(ğ‘–ğ‘‘) + ğ‘ƒ(ğ‘’ğ‘¥ğ‘)\\n(7)\\nThis combination occurs during both the downsampling and upsam-\\npling blocks of the model, where feature extraction and fusion take \\nplace. Rather than concatenating ğ‘ƒ(ğ‘’ğ‘¥ğ‘) and ğ‘ƒ(ğ‘–ğ‘‘), we adopt an additive \\nfusion approach, similar to [46], enabling multiple conditioning inputs \\nto blend smoothly. Given that CLIP provides 768-dimensional embed-\\ndings and Stable Diffusion is designed to process inputs of the 768-\\ndimensionality, concatenation would yield a 1536-dimensional vector, \\nrequiring unnecessary structural changes. By contrast, addition pre-\\nserves the original model architecture, ensuring efficient processing. \\nMoreover, as both prompts describe the same type of subject class \\n(e.g., a man or a woman), adding them reinforces the common identity \\nfeatures and prevents conflicts between the prompts.\\nBy using addition, the model learns the relationship between expres-\\nsive fused identity and expression token, ensuring that the generated \\nimage reflects both the subjectâ€™s facial features and the intended ex-\\npression. Upon receiving a specific expression token in the prompt, the \\nmodel synthesizes the desired expression while enhancing its intensity, \\nall while retaining the identity of the individual.\\n4.3. Instance 3: Context guider\\nTo effectively integrate context, we adopt a strategy similar to [32], \\nincorporating prior preservation loss with class-specific images gener-\\nated by instance 3. This helps the model retain the core knowledge \\nof the subject class (e.g., generating images of a specific subject, \\nlike a man/woman) even when fine-tuned on a particular individual. \\nApplying the prior preservation loss to the expression-inclusive images \\nensures that both the subjectâ€™s identity and facial expression are pre-\\nserved. This process enables the model to handle diverse environmental \\ncontexts such as changes in scenes or lighting without losing the \\nsubjectâ€™s identity or expression (shown in Fig. 2).\\n4.4. Fine-tuning and objective function\\nIn Instance 2, the main instance of the framework, we employ mul-\\ntiple loss functions to ensure both identity preservation and accurate \\nexpression generation.\\nIdentity Preservation Loss employed Perceptual loss (\\ue238percep) is \\nused to preserve the identity by comparing high-level feature extracted \\nby employing the VGG16 [67] pre-trained model. The Perceptual Loss \\nis defined as: \\n\\ue238percep = 1\\nğ‘\\nğ‘\\nâˆ‘\\nğ‘–=1\\nâ€–â€–â€–ğœ™ğ‘–(ğ¼gen) âˆ’ğœ™ğ‘–(ğ¼id)â€–â€–â€–\\n2\\n(8)\\nwhere ğœ™ğ‘– represents the feature maps from the ğ‘–th layer of the VGG16 \\nnetwork, ğ¼gen is the generated image, and ğ¼id is the identity image. ğ‘\\nis the number of feature maps considered.\\nExpression Crafting Loss composed of two losses for expression \\naccuracy by utilizing the ArcFace [52] model to extract features from \\nboth the reference expression and the generated images. Cosine Similar-\\nity Loss (\\ue238cosine), which measures the alignment of the feature vectors. \\nReconstruction Loss (\\ue238rec), which ensures pixel-level accuracy between \\nthe generated image and the reference expression image. The Cosine \\nSimilarity Loss is defined as: \\n\\ue238cosine = 1 âˆ’1\\nğ‘\\nğ‘\\nâˆ‘\\nğ‘–=1\\nğ‘“gen,ğ‘–â‹…ğ‘“exp,ğ‘–\\nâ€–ğ‘“gen,ğ‘–â€–â€–ğ‘“exp,ğ‘–â€–\\n(9)\\nThe Reconstruction Loss is defined as: \\n\\ue238rec = 1\\nğ‘\\nğ‘\\nâˆ‘\\nğ‘–=1\\n(ğ¼exp,ğ‘–âˆ’ğ¼gen,ğ‘–\\n)2\\n(10)\\nIn both losses, ğ‘– refers to the ğ‘–th feature, and the feature vectors ğ‘“gen\\nand ğ‘“ref are the features extracted from the generated and reference \\nimages, respectively. ğ‘ refers to the number of features.\\nPrior Preservation Loss (\\ue238prior) is introduced to preserve the \\nmodelâ€™s ability to generate diverse outputs and prevent overfitting \\nduring fine-tuning. This loss helps the model retain its generalization \\nInformation Fusion 125 (2026) 103431 \\n7 \\nM.S. Afgan et al.\\ncapabilities by ensuring that fine-tuning does not overly bias the model \\ntowards the specific subject. It is defined as: \\n\\ue238prior = Eğ‘¥âˆ¼ğ‘class\\n[\\nâ€–ğ‘“ğœƒ(ğ‘¥) âˆ’ğ‘“ğœƒpre(ğ‘¥)â€–2]\\n(11)\\nwhere ğ‘“ğœƒ represents the fine-tuned model, ğ‘“ğœƒpre represents the pre-\\ntrained model, and ğ‘class is the distribution of class prior images.\\nTo manage the combination of prompts and establish a coherent \\nconnection, the optimization is formulated as: \\nmin\\nğœ†1âˆ¶ğ‘‡\\n[\\n\\ue238clip\\n(\\nğ‘¿(ğ‘’ğ‘¥ğ‘)\\nexp , ğ‘¿(ğœ†)\\ngen, ğ‘ƒ(ğ‘’ğ‘¥ğ‘), ğ‘ƒ(ğ‘–ğ‘‘)\\n)]\\n(12)\\nwhere \\ue238clip measures how well the generated image ğ‘¿(ğœ†)\\ngen matches the \\nexpression image ğ‘¿(ğ‘’ğ‘¥ğ‘)\\nexp , given the influence of both prompts ğ‘ƒ(ğ‘’ğ‘¥ğ‘œ) and \\nğ‘ƒ(ğ‘–ğ‘‘).\\nOverall Loss: The total objective function combines these losses to \\nensure both identity preservation, expression accuracy, and the modelâ€™s \\ngeneralization. The total loss is given as: \\nğ¿total = ğ›¼\\ue238percep + ğ›½\\ue238cosine + ğ›¾\\ue238rec + ğ›¿\\ue238prior + ğœ‚\\ue238clip\\n(13)\\nwhere ğ›¼, ğ›½, ğ›¾, ğ›¿, and ğœ‚ are the weights assigned to each loss.\\n5. Implementation, experimental setup, and results\\nThis section demonstrates our proposed methodologyâ€™s implemen-\\ntation, experimental setup, and results.\\n5.1. Implementation\\nWe utilize the pre-trained weights of the â€˜â€˜stable-diffusion-xl-base-\\n1.0â€™â€™ model, publicly available through Hugging Face,1 due to its state-\\nof-the-art performance in image generation. The model generates 10 \\nfacial images for each expression in Instance 1, while extracting fea-\\ntures from key Block 2 (640 Ã— 32 Ã— 32) and Block 3 (1280 Ã— 16 Ã— 16). \\nWith a total of 40 steps, the features are extracted from time steps \\nranging from 0.7T to 0.3T, resulting in 16 total steps. For each set of 10 \\nimages, the extracted features total 160 for each block. The generated \\nimages, along with their corresponding features, are saved for later use \\nin the fusion mechanism.\\nUsing the same pre-trained model weights, we fine-tune the model \\nwith DDPM [22] over 400 timesteps. The schedule is split into 280 \\nsteps at 0.7T and 120 at 0.3T, totaling 400 steps. This process is \\nrepeated for each facial expression, and losses are calculated based \\non each expressionâ€™s reference images. We set the weights for the loss \\nfunctions as follows: ğ›¼= 0.55 for identity preservation, ğ›½= 0.3 for \\nexpression accuracy, ğ›¾= 0.05 for reconstruction quality, ğ›¿= 0.05\\nfor prior preservation, and ğœ‚= 0.05 for the clip loss. To prevent \\ncatastrophic forgetting during sequential fine-tuning, we employ mem-\\nory replay [68], periodically reusing previously generated images to \\npreserve the knowledge of earlier expressions.\\n5.2. Experimental setup\\nAll identity images collected from the CelebA-HQ dataset,2 which \\nprovides high-resolution facial images. The subjects are categorized \\ninto men and women. One sample image from each subjectâ€™s iden-\\ntity is presented in Fig. 6, with the top row labeled â€˜â€˜Identityâ€™â€™. The \\nexperimental setup utilized four NVIDIA GeForce RTX 4090 GPUs, \\neach equipped with 24 GB of VRAM. The training process for a single \\nidentity takes approximately 3 min. Our method generated images at a \\nresolution of 512 Ã— 512 pixels, requiring only 20 reverse diffusion steps \\nfor sampling [69]. The sampling time is remarkably efficient, taking \\napproximately 1 s per image.\\n1 https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0\\n2 http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\\n5.3. Fine-grained expression results\\nTo the best of our knowledge, no standalone or hybrid diffusion-\\nbased method in academia or industry has achieved this level of \\nfine-grained expression generation while preserving identity across eth-\\nnicities. Furthermore, no existing facial expression generation hybrid \\nmethods can accurately synthesize diverse expressions for a single \\nidentity, as extreme expressions often compromise identity. In contrast, \\nFaceExpr efficiently handles multiple expressions for a single identity \\nwithout sacrificing identity consistency. Fig. 6 highlights the capability \\nof FaceExpr to generate diverse expressions while maintaining identity \\nconsistency across different ethnic groups. Fig. 7 further demonstrates \\nFaceExprâ€™s effectiveness in generating accurate expressions across vari-\\nous head orientations, showcasing proficiency in action guidance while \\npreserving both expression and identity. Moreover, Fig. 8 illustrates \\nFaceExprâ€™s ability to incorporate contextual information, generating \\nfacial expressions that align with different settings, including attributes, \\naccessories, actions, and environments. The model successfully adapts \\nthe generated expressions to the given setting, proving its capability \\nto handle various contexts while preserving the subjectâ€™s identity and \\nexpression.\\n6. Comparative evaluation\\nThis section evaluates the proposed approach by comparing it with \\nexisting methods, categorized into standalone (entirely diffusion) and \\nhybrid (relying on external modules) approaches. The evaluation in-\\ncorporates performance metrics, user feedback, and ablation studies to \\nassess the effectiveness of the approach.\\n6.1. Evaluation metrics\\nThe evaluation is performed using five key metrics. Identity Preser-\\nvation (ID.) is assessed using the ArcFace model [52], which measures \\nthe euclidean distance between feature vectors of the original input \\nand generated faces. Values close to 0.5 indicate an exact match, while \\nvalues over 1.0 represent distinct identities, with lower values being \\nbetter. Semantic change (SemChg.) is evaluated using HRNet [70], \\nwhere values near 0.5 indicate minor changes, values closer to 1 reflect \\nmore significant changes, and higher values are preferable. Expression \\naccuracy (Exp.) is assessed using the VGGFace2 model [71] with a \\nResNet-50 backbone, where higher values indicate greater accuracy in \\ngenerating the intended expressions. Fidelity (IQ.) is measured using \\nthe Frechet Inception Distance (FID) [72], where lower values indi-\\ncate better fidelity and a closer alignment to the distribution of real \\nface. Text-to-Image Similarity (CLIP-T.) is evaluated using the CLIP \\nmodel [73] to measure the alignment between generated images and \\ninput text prompts, with higher values indicating stronger alignment. \\nFinally, Image-to-Text Similarity (CLIP-I.) is assessed using the CLIP \\nmodel [74] to evaluate how well the generated images align with \\ntheir textual descriptions, with higher values indicating better align-\\nment. Additionally, we evaluated our method in terms of complexity \\nby comparing the training and inference times, providing a more \\ncomprehensive analysis of FaceExpr.\\n6.2. Comparison with standalone approaches\\nWe compared FaceExpr with five standalone approaches: Textual In-\\nversion [31],3 DreamBooth [32],4 DisenBooth [33],5 Celeb-Basis [37],6 \\nand CIP-TIG [38].7 All methods were implemented to ensure a fair \\ncomparison.\\n3 https://github.com/rinongal/textual_inversion\\n4 https://github.com/XavierXiao/Dreambooth-Stable-Diffusion\\n5 https://github.com/forchchch/DisenBooth\\n6 https://github.com/ygtxr1997/CelebBasis\\n7 https://github.com/lyuPang/CrossInitialization\\nInformation Fusion 125 (2026) 103431 \\n8 \\nM.S. Afgan et al.\\nFig. 6. Visualization of FaceExprâ€™s performance with ethnic diversity and varied expressions. The images are generated using the prompt: â€™a front face photo of a [xyz*] \\nman/woman, {expression*}, where [xyz*] serves as an identity token unique to each individual and {expression*} represents various expression types, such as happy, sad, \\netc. This demonstrates FaceExprâ€™s proficiency in preserving identity and generating diverse facial expressions across multiple ethnicities.\\nFig. 7. Visualization of FaceExprâ€™s performance under extreme head orientations and various facial expressions. The face images are generated using the prompt: a \\n{orientation*} face photo of a [asianm1] man, {expression*} where {expression*} represents various expression types, such as happy, sad, etc., and {orientation*} \\nrepresents head action (e.g., front, side).\\nQualitative Comparison: To demonstrate the effectiveness of Face-\\nExpr, Fig. 9 presents visual comparisons with existing methods. For \\nclarity, we focus exclusively on expression prompts to highlight the \\nsemantic changes needed for accurate expression generation. As shown, \\nFaceExpr generates photorealistic personalized images with precise \\ndesired expressions. In contrast, optimization-based methods like Tex-\\ntual Inversion and DreamBooth struggle to modify facial semantics, \\nresulting in little to no change in expressions (e.g., columns 1â€“2). On \\nthe other hand, methods such as DisenBooth, Celeb-Basis, and CIP-TIG \\nproduce significant changes in facial semantics but compromise iden-\\ntity preservation and face fidelity (e.g., columns 3â€“5). In comparison, \\nFaceExpr generates personalized images that faithfully preserve both \\nthe identity and desired expression. Additional results in Fig. 11 further \\ndemonstrate FaceExprâ€™s ability to generate photorealistic images with \\ndiverse expressions, as well as variations in clothing, accessories, global \\nattributes, actions, and environmental settings.\\nQuantitative Comparison: In addition to the qualitative analy-\\nsis, we validate the performance of FaceExpr through a quantitative \\nInformation Fusion 125 (2026) 103431 \\n9 \\nM.S. Afgan et al.\\nFig. 8. Demonstrating FaceExprâ€™s ability to generate realistic facial expressions while preserving identity across various contexts, using the prompt given at the top where [W*] \\nand [M*] represent identity tokens labeled above each identity image. expression refers to facial expressions (e.g., happy, sad), attribute denotes features like hair, accessory such \\nas clothing, action describes poses or movements (e.g., standing), and context defines the environment (e.g., party, jungle).\\nTable 2\\nQuantitative comparison with standalone and hybrid methods are provided separately. The best \\nand second-best results are highlighted in bold and underlined, respectively. For ID., lower values \\n(closer to 0.5) indicate similarity, while values above 1.0 indicate distinct identity. For SemChg., \\nvalues near 0.5 signify minor changes, and values closer to 1 indicate more significant changes.\\n \\nMethod\\nID. â†“\\nExp. (%) â†‘\\nIQ. â†“\\nSemChg. â†‘\\nCLIP-T.â†‘\\nCLIP-I.â†‘ \\n \\nstandalone\\nTextual Inversion [31]\\n0.88\\n21.70\\n24.7\\n0.44\\n0.167\\n0.669  \\n \\nDreamBooth [32]\\n0.52\\n20.53\\n19.5\\n0.38\\n0.223\\n0.641  \\n \\nDisenbooth [33]\\n0.80\\n20.78\\n25.28\\n0.53\\n0.229\\n0.672  \\n \\nCeleb-Basis [37]\\n0.86\\n30.85\\n26.55\\n0.61\\n0.214\\n0.679  \\n \\nCIP-TIG [38]\\n1.20\\n67.62\\n26.70\\n0.65\\n0.228\\n0.666  \\n \\nFaceExpr (Ours)\\n0.78\\n85.25\\n19.62 \\n0.69\\n0.244\\n0.700  \\n \\nhybrid\\nDiffSFSR [39]\\n1.05\\n87.50\\n27.10\\n0.77\\n0.237\\n0.688  \\n \\nEmojiDiff [40]\\n0.85\\n83.33\\n24.50\\n0.65\\n0.234\\n0.673  \\n \\nW+ Adapter [41]\\n0.80\\n81.78\\n25.33\\n0.60\\n0.243\\n0.667  \\n \\nMasterWeaver [42]\\n0.82\\n82.44\\n23.70\\n0.62\\n0.231\\n0.678  \\n \\nFaceExpr (Ours)\\n0.78\\n85.25\\n19.62\\n0.69\\n0.244\\n0.700  \\nevaluation. The upper section of Table 2 compares FaceExpr with \\nstandalone methods. The results show that FaceExpr outperforms state-\\nof-the-art methods in expression accuracy (Exp.), semantic change \\n(SemChg.), text-to-image (CLIP-T.), and image-to-text (CLIP-I.) align-\\nment. Although FaceExpr does not achieve the highest identity (ID) and \\nimage quality (IQ) scores, this is not a limitation of the model but rather \\na direct result of the semantic modifications necessary for expression \\ngeneration, a point that reflects a fundamental trade-off in this task. \\nIn our approach, image quality (IQ) specifically refers to facial fidelity: \\nthe clarity and realism of identity-relevant facial regions. To synthesize \\nconvincing facial expressions, our model must alter key regions (such \\nas the mouth, eyes, and eyebrows) features that are not only crucial for \\nexpression but also integral to facial identity. These localized structural \\nshifts, while essential for accurate expression generation, partially dis-\\nrupt identity cues in those regions. Consequently, both ID and IQ scores \\nare impacted.\\nIn contrast, methods like DreamBooth achieve higher identity scores \\nby deliberately minimizing semantic changes to these expression-\\ncritical regions. However, this strategy inherently limits their ability \\nto produce expressive, rich images. Thus, the observed trade-off is not \\nincidental; it reflects an intrinsic property of expression-driven image \\ngeneration: the more accurately a model modifies facial structure to \\nrepresent expression, the more it risks altering the underlying identity.\\nIn summary, FaceExpr outperforms all competing standalone meth-\\nods. It achieves a strong balance across metrics: the second-best ID. \\nscore (0.78), the highest SemChg. score (0.69), the best Exp. ac-\\ncuracy (85.25%), and superior text-image alignment (CLIP-T. 0.244;\\nCLIP-I. 0.700).\\n6.3. Comparison with hybrid approaches\\nWe compared FaceExpr with four hybrid approaches: DiffSFSR [39], \\nEmojiDiff [40], W+ Adapter8 [41], and MasterWeaver9 [42]. Due to \\nthe lack of publicly available code for DiffSFSR and EmojiDiff, we \\nre-implemented both methods by closely following the architectural \\nguidelines and methodological details provided in their original papers. \\n8 https://github.com/csxmli2016/w-plus-adapter\\n9 https://github.com/csyxwei/MasterWeaver\\nInformation Fusion 125 (2026) 103431 \\n10 \\nM.S. Afgan et al.\\nFig. 9. Qualitative comparison with standalone methods. Existing approaches often fail to accurately generate the specified facial expressions. In this comparison, we omitted \\nadditional contextual details to focus solely on expression accuracy.\\nWhile certain implementation-specific nuances may be absent, our \\nreproduction aligns with the reported results for a fair comparison. Im-\\nportantly, unlike these hybrid methods, which require a large training \\ndataset and significant computational time for fine-tuning, our Face-\\nExpr framework operates as a standalone diffusion-based model, requir-\\ning no extra datasets and offering substantially lower computational \\ntime.\\nQualitative Comparison: To demonstrate the effectiveness of Face-\\nExpr, Fig. 10 presents visual comparisons with existing hybrid methods. \\nIn this comparison, we focus solely on expression prompts to high-\\nlight the effectiveness of our approach in enhancing facial close-up \\nimages for improved identity visualization. As shown, FaceExpr gen-\\nerates photorealistic personalized images with precise expressions. In \\ncontrast, hybrid methods such as EmojiDiff and DiffSFSR require a \\nreference image to guide the expression (shown in the top-right corner \\nof the generated image). These methods produce notable expression \\nchanges but compromise image fidelity and identity due to the use of \\nadditional learned embeddings (e.g., columns 1â€“2). Methods like W+ \\nAdapter and MasterWeaver also generate better expression images but \\nstruggle to accurately capture extreme expressions, such as surprise \\nand shock, compromising both identity preservation and face fidelity \\n(e.g., columns 3â€“4). In comparison, FaceExpr generates personalized \\nimages that preserve identity with the highest fidelity. Additional re-\\nsults in Fig. 11 further demonstrate FaceExprâ€™s ability to generate \\nphotorealistic images with diverse expressions, as well as variations \\nin clothing, accessories, global attributes, actions, and environmental \\nsettings.\\nQuantitative Comparison: In addition to the qualitative analysis, \\nwe assess the performance of FaceExpr through a quantitative evalu-\\nation. The lower portion of Table 2 presents a comparison between \\nFaceExpr and hybrid approaches. The findings indicate that FaceExpr \\nsurpasses hybrid methods in identity (ID.), image fidelity (IQ.), text-\\nto-image (CLIP-T.), and image-to-text (CLIP-I.) alignment. However, \\nFaceExprâ€™s expression accuracy (Exp.) and semantic change (SemChg.) \\nscores are not the highest, which can be attributed to the absence \\nof reference images for expression guidance and reliance solely on \\ndiffusion models.\\nIn summary, FaceExpr outperforms all competing hybrid methods, \\nachieving a strong balance across metrics. It delivers the best ID. score \\n(0.78), the second-best SemChg score (0.69), Exp. accuracy (85.25%), \\nand superior image fidelity IQ. score (19.62). Additionally, it excels in \\ntext-image a alignment (CLIP-T. 0.244; CLIP-I. 0.700).\\n6.4. Comparison of computational complexity\\nAdditionally, we conducted a comprehensive comparison of our \\nmethod with others. Regarding training, all methods were implemented \\non the same machine used for FaceExpr to ensure consistency in the \\nevaluation, as detailed in Table 3. Textual Inversion requires 8 min \\nand 1k steps to optimize embeddings for a single identity, DreamBooth \\ntakes 15 min and 800 steps to fine-tune the model for a single identity, \\nDisenbooth demands approximately 70 h and 12k steps, Celeb-Basis \\nneeds 9 min and 800 steps, and CIP-TIG takes around 30 h and 10k \\nsteps. Among the hybrid methods, W+ Adapter requires 120 h and 8k \\nsteps to train, MasterWeaver takes 133 h and 100k steps, EmojiDiff de-\\nmands 60 h and 10k steps, and DiffSFSR requires 130 h and 100k steps. \\nIn contrast, FaceExpr demonstrates exceptional efficiency, requiring\\nonly 3 min and 400 steps for fine-tuning, highlighting a substantial \\nreduction in training time.\\nFor inference, Textual Inversion takes 200 s with 50â€“100 steps, \\nDreamBooth requires 180 s with 50â€“100 steps, and Disenbooth de-\\nmands 180 s with 50â€“100 steps. Celeb-Basis requires 200 s with 100 \\nsteps, and CIP-TIG takes 400 s with 200 steps. Hybrid methods like \\nW+ Adapter require 8 s with 50 steps, MasterWeaver takes 4 s with 50 \\nsteps, EmojiDiff takes 3 s with 80 steps, and DiffSFSR needs 2 s with \\n50 steps. FaceExpr, however, outperforms all by generating an image \\nInformation Fusion 125 (2026) 103431 \\n11 \\nM.S. Afgan et al.\\nFig. 10. Qualitative comparison of FaceExpr with hybrid approaches. While hybrid models rely on pre-trained learned embeddings for expression guidance, FaceExpr generates \\nexpressions solely from text input, outperforming hybrid models by producing higher-fidelity facial expressions while preserving identity.\\nTable 3\\nQuantitative comparison of FaceExpr with hybrid and standalone approaches, highlighting its lower \\ncomplexity and more efficient training and inference times compared to the hybrid methods.\\n Methods\\nComplexity\\nTraining\\nInference\\n \\nTime (m)/Steps\\nTime (s)/Steps \\n Textual Inversion [31]\\nLow (Optimize Single Embedding)\\n8/1000\\n200/50-100\\n DreamBooth [32]\\nHigh (Full Model Fine-Tuning)\\n15/800\\n180/50-100\\n Disenbooth [33]\\nHigh (Feature Disentanglement Framework)\\n4200/12k\\n180/50-100\\n Celeb-Basis [37]\\nHigh (Parameter Learning)\\n9/800\\n200/100\\n CIP-TIG [38]\\nMedium (Improved Initialization)\\n1800/10k\\n400/200\\n DiffSFSR [39]\\nMedium (learned embedding + Fine-Tuning)\\n7800/100k\\n2 /50\\n EmojiDiff [40]\\nHigh (learned embedding + Fine-Tuning)\\n3,600 /10k\\n3/80\\n W+ Adapter [41]\\nHigh (learned embedding + Fine-Tuning)\\n7,200/8k\\n8/50\\n MasterWeaver [42]\\nHigh (learned embedding + Fine-Tuning)\\n8000/100k\\n4/50\\n FaceExpr\\nLow (Feature Fusion + Fine-Tuning)\\n3/400\\n1 /20\\nin just 1 s with only 20 steps, underscoring its superior efficiency in \\nboth training and inference. This makes FaceExpr the most suitable so-\\nlution for real-time dynamic facial expression generation across diverse \\ncontexts.\\n6.5. User feedback study\\nThis section explains the quantitative analysis presented in Table 4. \\nThe analysis is based on feedback from 89 randomly selected partici-\\npants (51 males, 38 females, aged 18â€“45). Each participant evaluated \\nimages generated by our FaceExpr alongside various standalone and hy-\\nbrid methods, focusing on four key metrics by answering the following \\nquestions:\\n1. Identity Preservation: Does the generated image resemble the \\nperson in the Original ID image?\\n2. Expression Accuracy: Does the expression in the generated \\nimage match the desired expression?\\n3. Realism: Does the face in the generated image appear natural \\nand lifelike?\\n4. Image Quality: Is the overall generated image (including the \\nface, background, and lighting) clear and well-rendered?\\nStatistical significance was assessed using a ğ‘-value threshold of 0.05. \\nTextual Inversion and DreamBooth were excluded from the study due \\nto expression accuracy falling below 30%.\\nIdentity Preservation: FaceExpr achieves the highest score of 4.50, \\nsignificantly outperforming all other methods. The closest competitor, \\nW+ Adapter, scores 4.22 (p = 0.12), but the difference is not statis-\\ntically significant. MasterWeaver follows with 3.98 (p = 0.18), but it \\nlags behind FaceExpr by a notable margin. Methods like Disenbooth \\nand Celeb-Basis perform poorly in this category, with scores of 2.22 \\nand 3.15, respectively.\\nExpression Accuracy: DiffSFSR achieves the highest score of 4.22 \\n(p = 0.292), but FaceExpr is a close second with 4.13. EmojiDiff also \\nperforms well with 3.70 (ğ‘< 0.05), but it does not surpass FaceExpr. \\nW+ Adapter and MasterWeaver show competitive results with 3.87 and \\n3.82, respectively, but they fall short of FaceExprâ€™s performance.\\nRealism: FaceExpr leads with a score of 4.44, followed by W+ \\nAdapter with 4.31 (p = 0.23) and MasterWeaver with 4.17 (ğ‘< 0.05). \\nInformation Fusion 125 (2026) 103431 \\n12 \\nM.S. Afgan et al.\\nFig. 11. Comprehensive comparison of FaceExpr with various standalone and hybrid approaches using detailed prompts that include attributes, accessories, actions, and \\nenvironments/scenes along with desired facial expressions. FaceExpr is the most effective in preserving identity integrity while achieving the highest expression accuracy and \\ngenerating realistic, high-quality images.\\nTable 4\\nUser study comparing FaceExpr with standalone and hybrid methods. The best and second-best results are \\nhighlighted in bold and underlined, respectively. This quantitative comparison includes ANOVA tests, where \\na ğ‘-value less than 0.05 indicates a statistically significant difference in performance from FaceExpr.\\n Methods\\nIdentity. â†‘\\nExpression. â†‘\\nRealism. â†‘\\nImage Quality. â†‘ \\n Disenbooth [33]\\n2.22 (ğ‘< 0.001)\\n0.63 (ğ‘< 0.001)\\n3.10 (ğ‘< 0.001)\\n3.03 (ğ‘< 0.001)  \\n Celeb-Basis [37]\\n3.15 (ğ‘< 0.05)\\n2.78 (p = 0.145)\\n3.64 (ğ‘< 0.05)\\n3.65 (p = 0.077) \\n CIP-TIG [38]\\n2.86 (p = 0.086)\\n3.00 (ğ‘< 0.05)\\n3.52 (p = 0.055)\\n3.52 (ğ‘< 0.05)  \\n DiffSFSR [39]\\n3.81 (p=0.043)\\n4.22 (ğ‘< 0.05)\\n3.60 (ğ‘< 0.05)\\n3.65 (ğ‘< 0.05)  \\n EmojiDiff [40]\\n3.10 (p = 0.083)\\n3.70 (ğ‘< 0.05)\\n3.89 (p = 0.12)\\n3.82 (ğ‘< 0.05)  \\n W+ Adapter [41]\\n4.22 (p = 0.12)\\n3.87 (ğ‘< 0.05)\\n4.31 (p = 0.09)\\n3.94 (ğ‘< 0.05)  \\n MasterWeaver [42]\\n3.98 (p = 0.18)\\n3.82 (p = 0.15)\\n4.17 (p = 0.15)\\n4.15 (p = 0.183) \\n FaceExpr\\n4.50\\n4.13\\n4.44\\n4.50\\n \\nWhile W+ Adapter and MasterWeaver show strong performance, they \\ndo not surpass FaceExpr. Methods like Disenbooth and Celeb-Basis \\nperform significantly worse, with scores of 3.10 and 3.64, respectively.\\nImage Quality: FaceExpr achieves the highest score of 4.50, fol-\\nlowed by MasterWeaver with 4.15 (p = 0.183) and W+ Adapter with \\n3.94 (ğ‘< 0.05). EmojiDiff and CIP-TIG also show competitive results \\nwith 3.82 and 3.52, respectively, but they do not match FaceExprâ€™s \\nperformance. Disenbooth and Celeb-Basis lag behind with scores of \\n3.03 and 3.65, respectively.\\nIn summary, FaceExpr establishes a new benchmark for facial ex-\\npression synthesis, surpassing existing methods in identity preservation, \\nrealism, and image quality, while remaining highly competitive in \\nexpression synthesis. It outperforms others across all metrics, achieving\\n90%â€“95% identity retention, 80%â€“84% expression accuracy, and \\nsuperior realism and image quality.\\n6.6. Ablation study\\nWe structure our evaluation around two analytical perspectives. \\nFirst, the component ablation study isolates the effects of our novel \\nfeature fusion strategy and the integration of the additional prompt \\nembedding. Next, we provide an instance-based evolution analysis built \\nupon our innovative three-instance framework. This analysis traces \\nthe progressive influence of each instance across various evaluation \\nmetrics, examining their individual contributions.\\n6.6.1. Component ablation study\\nIn this subsection of the ablation study, we analyze two key com-\\nponents: the impact of feature fusion and the impact of the expression \\nprompt within the U-Net module. we analyze four time-step feature fu-\\nsion scenarios to assess the impact on identity preservation, expression \\naccuracy, and semantic changes.\\nFeature Fusion Effect: Fig. 12(a) visualizes different fusion strate-\\ngies across four scenarios (each row (1 to 4) corresponds to a specific \\nfusion configuration.), and the left portion of Table 5 provides the \\nimpact of feature fusion on identity preservation, expression accuracy, \\nand semantic change.\\nâ€¢ Scenario 1: Feature fusion timing significantly influences seman-\\ntic change and identity preservation. When fusion occurs from \\n(T â†’0T), semantic change is maximized (SemChg = 0.70), lead-\\ning to the highest expression accuracy (Exp = 84.50%) but poor \\nInformation Fusion 125 (2026) 103431 \\n13 \\nM.S. Afgan et al.\\nFig. 12. Ablation study on feature fusion and prompt embeddings across different time steps: (a) Impact of feature fusion, with the fourth scenario (0.7ğ‘‡â†’0.3ğ‘‡) showing the \\noptimal alignment of expressions and identity. (b) Effect of expression prompt embedding, with the (0.7ğ‘‡â†’0.3ğ‘‡) interval achieving improved expression accuracy and balanced \\nidentity preservation.\\nTable 5\\nAblation study on attention-based feature fusion and prompt embeddings: Quantitative evaluation \\nof the trade-off between identity preservation and expression accuracy across denoising time steps. \\n \\nFeature Fusion Impact\\nPrompt Embedding Impact\\n \\nTime-step\\nID.â†“\\nExp.(%)â†‘\\nSemChg.â†‘\\nID.â†“\\nExp.(%)â†‘\\nSemChg.â†‘ \\n \\nScenario\\nT â†’ 0T\\n1.3\\n84.50\\n0.70\\n0.90\\n85.2\\n0.65\\n \\n \\n0.7T â†’ 0T\\n0.98\\n89.36\\n0.78\\n0.78\\n82.0\\n0.62\\n \\n \\nT â†’ 0.3T\\n1.45\\n86.6\\n0.75\\n0.83\\n89\\n0.90\\n \\n \\n0.7T â†’ 0.3T\\n0.78\\n82.0\\n0.62\\n0.78\\n85.5\\n0.69\\n \\nidentity preservation (ID = 1.3). At early denoising stages (higher \\ntimesteps, closer to ğ‘‡), the model prioritizes broad structural and \\nsemantic elements, treating expression features as fundamental \\ncomponents rather than modifications to the desired identity. \\nConsequently, identity drift occurs as the model reconstructs an \\nidentity influenced by the extracted expressive features rather \\nthan preserving the original, as shown in the figure, row 1.\\nâ€¢ Scenario 2: Adjusting fusion to (0.7T â†’0T) improves identity \\npreservation (ID = 0.98) while achieving strong semantic change \\n(SemChg = 0.78) and expression accuracy (Exp = 89.36%). Here, \\nthe model first establishes identity as a core feature before fusion \\nbegins at 0.7T. As denoising progresses, expression adaptation \\ncompetes with identity retention, requiring a balance between \\nboth, as seen in row 2 of the same figure. Despite strong feature \\nfusion, this setup was not selected due to relatively lower identity \\npreservation.\\nâ€¢ Scenario 3: Fusion from (T â†’0.3T) further decreases iden-\\ntity preservation (ID = 1.45), with moderate semantic change \\n(SemChg = 0.75) and reduced expression accuracy (Exp = 86.6%). \\nSimilar to the scenario 1: (T â†’0T) case, early fusion leads to \\nidentity deviation, as illustrated in row 3 of the figure.\\nâ€¢ Scenario 4: The most balanced configuration is achieved when \\nfusion is applied from (ğŸ.ğŸ•ğ“â†’ğŸ.ğŸ‘ğ“), yielding optimal semantic \\nchange ( ğ’ğğ¦ğ‚ğ¡ğ = ğŸ.ğŸ”ğŸ), good expression accuracy (ğ„ğ±ğ©= ğŸ–ğŸ.ğŸ%), \\nand improved identity preservation (ğˆğƒ= ğŸ.ğŸ•ğŸ–). Here, the model \\nfirst establishes identity, then integrates expression features while \\nmaintaining constraints, ultimately refocusing on the desired \\nidentity at 0.3T. This adaptive balance ensures expressive feature \\nalignment without significant identity loss, as demonstrated in \\nrow 4.\\nPrompt Embedding Effect: In this part, we evaluate the impact of \\nintegrating additional prompt embeddings, which aim to establish the \\nconnection between expressive fused identity and expression-specific \\nprompt. Fig. 12(b) presents four different time-step scenarios, with each \\nrow (1 to 4) representing a distinct configuration. The right section \\nof Table 5 quantifies the effects of prompt embedding on identity \\npreservation, expression accuracy, and semantic consistency across \\nthese configurations.\\nâ€¢ Scenario 1: We first evaluate the impact of embedding prompts \\nacross all time steps (T â†’0T). The results show a clear trend: \\nIdentity Preservation (ID = 0.90) decreases by 0.12 (worse than \\nthe target ID = 0.78), while Expression Accuracy (Exp = 85.2%) \\nincreases by 3.2% (compared to the target 82.0%), and Semantic \\nChange (SemChg = 0.65) rises by 0.03 (compared to the target \\n0.62). This indicates the strong influence of additional prompt \\nembeddings on the generated face, similar to fusion scenario 1. \\nDuring early denoising, the sample is still noisy, and the model \\ntreats the additional prompt embedding as an intrinsic part of the \\nexternal condition, leading to identity drift (see figure, row 1).\\nâ€¢ Scenario 2: To mitigate this, we explored the optimal time-step \\nrange that establishes a connection without excessively influenc-\\ning expressive fused identity. Embedding prompts from (0.7T â†’\\n0T) yielded expected results: identity preservation (ID = 0.78), ex-\\npression intensity (Exp = 82.0%), and semantic change (SemChg =\\n0.62), as shown in figure, row 2.\\nâ€¢ Scenario 3: Next, we hypothesized that refining the time-step \\nrange could enhance expression intensity without compromising \\nidentity. Embedding prompts from (T â†’0.3T) resulted in a \\nslight decrease in identity preservation (ID = 0.83, a 0.05 drop \\nfrom the target 0.78), increased semantic change (SemChg =\\n0.90, up by 0.28 from the target 0.62), and improved expression \\naccuracy (Exp = 89.0%, up by 7.0% from the target 82.0%). \\nHowever, identity preservation dropped beyond the acceptable \\nrange, causing a shift in identity, as seen in the figure, row 3.\\nâ€¢ Scenario 4: To address this, we further explored and embedded \\nprompts from (ğŸ.ğŸ•ğ“â†’ğŸ.ğŸ‘ğ“), achieving a better balance: iden-\\ntity preservation (ğˆğƒ= ğŸ.ğŸ•ğŸ–), expression accuracy (ğ„ğ±ğ©= ğŸ–ğŸ“.ğŸ“%, \\na ğŸ‘.ğŸ“% increase), and semantic change (ğ’ğğ¦ğ‚ğ¡ğ = ğŸ.ğŸ”ğŸ—, exceeding \\nthe target by ğŸ.ğŸğŸ•). As shown in row 4 of the figure, this configura-\\ntion successfully enhanced expression accuracy while preserving \\nidentity. Among all configurations, the first was ineffective, and \\nthe last provided the best balance between identity preserva-\\ntion and enhanced expression accuracy while establishing the \\nconnection.\\nInformation Fusion 125 (2026) 103431 \\n14 \\nM.S. Afgan et al.\\nFig. 13. Ablation study of the three instances in FaceExpr: Visualizing the quantitative impact of each instance, (1) Inst. 2 (w/o Fusion & Text Embedding), (2) Inst. 1 + Inst. 2 \\n(w/ Fusion & Text Embedding), and (3) Inst. 1 + Inst. 2 + Inst. 3 (Prior Preservation).\\nTable 6\\nInstance-Based Ablation Study: Quantitative Evaluation of the FaceExpr Three-Instance Design and Its Impact \\non Performance Metrics.\\n Instance Configuration\\nFusion\\n Text Emb.\\nID. â†“\\nSemChg. â†‘\\nExp. â†‘\\nCLIP-T. â†‘\\nIQ. â†“ \\n Inst.2\\nÃ—\\nÃ—\\n0.52\\n0.32\\nâ€“\\n0.128\\n19.20 \\n Inst.2 + Inst.3\\nâœ“\\nâœ“\\n0.52\\n0.30\\nâ€“\\n0.223\\n19.62 \\n Inst.2 + Inst.1\\nâœ“\\nâœ“\\n0.78\\n0.72\\n87.77\\n0.137\\n20.17 \\n Inst.2 + Inst.1 + Inst.3 \\nâœ“\\nâœ“\\n0.78\\n0.69\\n85.25\\n0.244\\n19.62 \\n6.6.2. Instance-based ablation study\\nBeyond a component-wise ablation, we delve into the behavioral \\ndynamics of each of the three model instances within our innovative \\nFaceExpr framework. Each instance encapsulates a distinct generative \\nbehavior pathway, contributing uniquely to the modelâ€™s output charac-\\nteristics. The ablation study, as illustrated in Fig. 13 and quantitatively \\nsummarized in Table 6, demonstrates the cumulative impact of in-\\ntegrating these components. The evaluation spans key performance \\naxes, including identity preservation, semantic change, expression ac-\\ncuracy, contextual-text alignment, and face fidelity. This progressive \\nanalysis reveals how each instance contributes to balancing these often \\ncompeting objectives, offering insights into the internal trade-offs and \\nsynergies that emerge during the generation process.\\nInstance 2 (w/o Fusion & Text Embedding): The model operates \\nwith only Instance 2 (without fusion or textual embedding), and it \\nshows no capability in both expression synthesis and context aware-\\nness. Although identity preservation remains relatively strong (ID score: \\n0.52), the absence of facial expression cues (no expression score avail-\\nable) and minimal semantic alteration (SemChg: 0.32) demonstrates the \\nmodelâ€™s inability to deviate meaningfully from the input identity. Ad-\\nditionally, the low CLIP-Text alignment score (CLIP-T: 0.128) indicates \\nthat the generated images lack semantic coherence with the intended \\ntextual prompts related to context. While face fidelity (IQ: 19.20) is \\nrelatively good due to minimal visual change, it reflects the lack of \\nmeaningful transformation in the facial semantics, not the successful \\ngeneration of desired expression (See row 1).\\nInstance 2 (w/o Fusion & Text Embedding) + Instance 3: This \\nconfiguration incorporates Instance 3 without leveraging fusion or text \\nembeddings. While identity preservation remains the same (ID: 0.52), \\nthe model exhibits almost no semantic change (SemChg: 0.30), indicat-\\ning no adaptability to expression or attribute modifications. Although \\nCLIP-Text alignment increases to 0.223, implying enhanced consistency \\nwith the prompt (See row 2).\\nInstance 2 (w/ Fusion & Text Embedding) + Instance 1: In-\\ntroducing feature fusion and text embeddings in this configuration \\nsignificantly boosts the modelâ€™s ability to generate meaningful expres-\\nsions (Exp.: 87.77) and improves semantic shift (SemChg: 0.72). These \\nenhancements, however, come at the cost of identity preservation, \\nwhich drops to 0.78. This trade-off is expected and can be attributed \\nto the nature of facial expression changes, which inherently alter \\ncertain identity-linked features such as muscle movement and localized \\nstructure. Face fidelity (IQ: 20.17) also slightly degraded, reflecting the \\nvisual complexity introduced by more expressive outputs. Importantly, \\nthe decrease in CLIP-Text alignment (CLIP-T: 0.137) can be attributed \\nto a lack of context (See row 3).\\nInstance 2 + Instance 1 + Instance 3 (w/ Prior Preservation):\\nThe full framework configuration, which incorporates all Instances, \\nachieves the most balanced performance. Identity preservation (ID) \\nremains consistent with a score of 0.78, while expression accuracy \\n(Exp.) reaches 85.25 and semantic shift (SemChg) registers at 0.69, \\nboth reflecting slight reductions. This modest decline is attributed to \\nthe regularizing effect of the prior preservation loss, which counteracts \\nthe distributional shift caused by fine-tuning and pulls the model \\ncloser to the original distribution. Notably, CLIP-T alignment reaches its \\nhighest value of 0.244, indicating a stronger correspondence between \\nthe generated images and textual prompts due to better alignment in \\nboth context and expression. Furthermore, face fidelity (IQ) improves \\nwith a score of 19.62, which suggests that prior preservation effectively \\nmitigates visual degradation while preserving fine facial details (See \\nrow 4).\\nDiscussion Behind the Three-Instance Design: The three-instance \\ndesign plays a pivotal role in optimizing the performance and spe-\\ncialization of our model by clearly segmenting each task into distinct \\nInformation Fusion 125 (2026) 103431 \\n15 \\nM.S. Afgan et al.\\ncomponents. Each instance is tailored to handle a specific responsibil-\\nity, ensuring that the model remains efficient and prevents overfitting. \\nInstance 1 focuses on feature extraction and generating multiple diverse \\nexpressions along with corresponding features, providing the base for \\nfurther processing. Instance 2 is responsible for fine-tuning, allow-\\ning the model to specialize in generating specific expressions without \\ncompromising facial identity. This separation ensures that identity \\ndiversity is maintained while facilitating high-quality expression gen-\\neration. Instance 3 introduces prior preservation, acting as a safeguard \\nagainst catastrophic forgetting, which ensures the model retains crucial \\nknowledge from its base training. This division not only minimizes \\ntask conflicts but also enhances the modelâ€™s robustness and ability to \\ngeneralize, especially when handling unseen prompts.\\n7. Conclusion and future directions\\nIn this paper, we proposed the FaceExpr an innovative three-\\ninstance framework, based on a person-specific fine-tune approach that \\nemploys only a diffusion model for generating user-defined human \\nfacial expressions with customizable diverse settings, offering enhanced \\ncontrol for achieving personalization. To accomplish this, we introduce \\na unique and exceptionally novel feature fusion with an attention-\\nbased mechanism, along with the integration of additional expression \\ntext embeddings into the U-Net denoising module. FaceExpr effec-\\ntively aligns identity and expression features, while the inclusion of \\nexpression crafting loss ensures seamless blending of identity with ac-\\ncurate expression. Extensive experiments demonstrated that FaceExpr \\nachieves 90%â€“95% identity preservation and 84â€“86.5% expression ac-\\ncuracy across diverse ethnic groups, outperforming existing standalone \\nand hybrid methods. This highlights its potential for personalized con-\\ntent generation in digital storytelling, immersive virtual worlds, and \\nresearch applications.\\nFuture work could extend FaceExpr to control full-body attributes, \\nenabling more comprehensive personalization for a wide range of \\napplications. These advancements would improve the generation of \\nhigh-quality, customizable human datasets, advancing research in areas \\nlike person re-identification and face recognition.\\nCRediT authorship contribution statement\\nMuhammad Sher Afgan: Writing â€“ original draft, Visualization, \\nMethodology, Conceptualization. Bin Liu: Writing â€“ review & editing, \\nValidation, Supervision, Resources, Investigation. Mamoona Naveed \\nAsghar: Writing â€“ review & editing. Wajahat Khalid: Writing â€“ review \\n& editing, Visualization, Formal analysis. Kai Zou: Writing â€“ review \\n& editing, Visualization, Methodology. Dianmo Sheng: Visualization, \\nValidation, Investigation.\\nDeclaration of competing interest\\nThe authors declare that they have no known competing finan-\\ncial interests or personal relationships that could have appeared to \\ninfluence the work reported in this paper.\\nAppendix A. Supplementary data\\nSupplementary material related to this article can be found online \\nat https://doi.org/10.1016/j.inffus.2025.103431.\\nData availability\\nThe code supporting the findings of this study is openly available in \\nGitHub, https://github.com/MSAfganUSTC/FaceExpr.git.\\nReferences\\n[1] T.T. Nguyen, Q.V.H. Nguyen, D.T. Nguyen, D.T. Nguyen, T. Huynh-The, S. \\nNahavandi, T.T. Nguyen, Q.-V. Pham, C.M. Nguyen, Deep learning for deepfakes \\ncreation and detection: A survey, Comput. Vis. Image Underst. 223 (2022) \\n103525.\\n[2] X. Ke, B. Lin, W. Guo, LocalFace: Learning significant local features for deep \\nface recognition, Image Vis. Comput. 123 (2022) 104484.\\n[3] R. Sharma, A. Ross, Periocular biometrics and its relevance to partially masked \\nfaces: A survey, Comput. Vis. Image Underst. 226 (2023) 103583.\\n[4] C. Bisogni, L. Cimmino, M. De Marsico, F. Hao, F. Narducci, Emotion recognition \\nat a distance: The robustness of machine learning based on hand-crafted facial \\nfeatures vs deep learning models, Image Vis. Comput. 136 (2023) 104724, \\nhttp://dx.doi.org/10.1016/j.imavis.2023.104724.\\n[5] J. Zhang, W. Wang, X. Li, Y. Han, Recognizing facial expressions based on \\npyramid multi-head grid and spatial attention network, Comput. Vis. Image \\nUnderst. 244 (2024) 104010.\\n[6] F. Boutros, V. Struc, J. Fierrez, N. Damer, Synthetic data for face recognition: \\nCurrent state and future prospects, Image Vis. Comput. 135 (2023) 104688, \\nhttp://dx.doi.org/10.1016/j.imavis.2023.104688.\\n[7] Z. Guo, G. Yang, D. Wang, D. Zhang, A data augmentation framework by mining \\nstructured features for fake face image detection, Comput. Vis. Image Underst. \\n226 (2023) 103587.\\n[8] P. Zhou, Y. Zhang, Synthetic data generation for facial recognition: A comprehen-\\nsive survey, Image Vis. Comput. 105 (2021) 66â€“82, http://dx.doi.org/10.1016/\\nj.imavis.2020.08.007.\\n[9] K. Zhang, Z. Zhang, X. Wu, J. Zhang, The importance of diversity in datasets \\nfor facial expression recognition, Image Vis. Comput. 80 (2018) 103â€“112, http:\\n//dx.doi.org/10.1016/j.imavis.2018.03.002.\\n[10] D.P. Kingma, Auto-encoding variational bayes, 2013, arXiv preprint arXiv:1312.\\n6114.\\n[11] D. Rezende, S. Mohamed, Variational inference with normalizing flows, in: \\nInternational Conference on Machine Learning, PMLR, 2015, pp. 1530â€“1538.\\n[12] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. \\nCourville, Y. Bengio, Generative adversarial nets, Adv. Neural Inf. Process. Syst. \\n27 (2014).\\n[13] T. Karras, T. Aila, S. Laine, J. Lehtinen, Progressive growing of gans for improved \\nquality, stability, and variation, 2017.\\n[14] A. Brock, J. Donahue, K. Simonyan, Large scale GAN training for high fidelity \\nnatural image synthesis, 2018.\\n[15] T. Zhou, Q. Li, H. Lu, Q. Cheng, X. Zhang, GAN review: Models and medical \\nimage fusion applications, Inf. Fusion 91 (2023) 134â€“148.\\n[16] M.N. Asghar, N. Kanwal, B. Lee, M. Fleury, M. Herbst, Y. Qiao, Visual surveil-\\nlance within the EU general data protection regulation: A technology perspective, \\nIEEE Access 7 (2019) 111709â€“111726, http://dx.doi.org/10.1109/ACCESS.2019.\\n2934226.\\n[17] H. Sun, S. Wu, L. Ma, Adversarial attacks on GAN-based image fusion, Inf. Fusion \\n108 (2024) 102389.\\n[18] G. Kwon, J.C. Ye, Diagonal attention and style-based gan for content-style disen-\\ntanglement in image generation and translation, in: Proceedings of the IEEE/CVF \\nInternational Conference on Computer Vision, 2021, pp. 13980â€“13989.\\n[19] C. Fu, Y. Hu, X. Wu, G. Wang, Q. Zhang, R. He, High-fidelity face manipulation \\nwith extreme poses and expressions, IEEE Trans. Inf. Forensics Secur. 16 (2021) \\n2218â€“2231, http://dx.doi.org/10.1109/TIFS.2021.3050065.\\n[20] Y. Han, J. Yang, Y. Fu, Disentangled face attribute editing via instance-aware \\nlatent space search, 2021, arXiv:2105.12660.\\n[21] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, S. Ganguli, Deep unsupervised \\nlearning using nonequilibrium thermodynamics, in: International Conference on \\nMachine Learning, PMLR, 2015, pp. 2256â€“2265.\\n[22] J. Ho, A. Jain, P. Abbeel, Denoising diffusion probabilistic models, Adv. Neural \\nInf. Process. Syst. 33 (2020) 6840â€“6851.\\n[23] Y. Song, J. Sohl-Dickstein, D.P. Kingma, A. Kumar, S. Ermon, B. Poole, \\nScore-based generative modeling through stochastic differential equations, 2020.\\n[24] B. Kawar, S. Zada, O. Lang, O. Tov, H. Chang, T. Dekel, I. Mosseri, M. Irani, \\nImagic: Text-based real image editing with diffusion models, in: Proceedings of \\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, \\npp. 6007â€“6017.\\n[25] A. Lugmayr, M. Danelljan, A. Romero, F. Yu, R. Timofte, L.V. Gool, RePaint: \\nInpainting using denoising diffusion probabilistic models, 2022, URL: https:\\n//arxiv.org/abs/2201.09865, arXiv:2201.09865.\\n[26] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, B. Ommer, High-resolution \\nimage synthesis with latent diffusion models, in: Proceedings of the \\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. \\n10684â€“10695.\\n[27] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E.L. Denton, K. Ghasemipour, R. \\nGontijo Lopes, B. Karagol Ayan, T. Salimans, et al., Photorealistic text-to-image \\ndiffusion models with deep language understanding, Adv. Neural Inf. Process. \\nSyst. 35 (2022) 36479â€“36494.\\n[28] K. Sung-Bin, A. Senocak, H. Ha, A. Owens, T.-H. Oh, Sound to visual scene \\ngeneration by audio-to-visual latent alignment, 2023, URL: https://arxiv.org/abs/\\n2303.17490, arXiv:2303.17490.\\nInformation Fusion 125 (2026) 103431 \\n16 \\nM.S. Afgan et al.\\n[29] K. Wang, S. Deng, J. Shi, D. Hatzinakos, Y. Tian, AV-DiT: Efficient audio-\\nvisual diffusion transformer for joint audio and video generation, 2024, URL: \\nhttps://arxiv.org/abs/2406.07686, arXiv:2406.07686.\\n[30] F. Daneshfar, A. Bartani, P. Lotfi, Image captioning by diffusion models: A \\nsurvey, Eng. Appl. Artif. Intell. 138 (2024) 109288, http://dx.doi.org/10.1016/j.\\nengappai.2024.109288, URL: https://www.sciencedirect.com/science/article/pii/\\nS0952197624014465.\\n[31] R. Gal, Y. Alaluf, Y. Atzmon, O. Patashnik, A.H. Bermano, G. Chechik, D. Cohen-\\nOr, An image is worth one word: Personalizing text-to-image generation using \\ntextual inversion, 2022.\\n[32] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, K. Aberman, Dreambooth: \\nFine tuning text-to-image diffusion models for subject-driven generation, in: \\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern \\nRecognition, 2023, pp. 22500â€“22510.\\n[33] H. Chen, Y. Zhang, X. Wang, X. Duan, Y. Zhou, W. Zhu, DisenBooth: Disentangled \\nparameter-efficient tuning for subject-driven text-to-image generation, 2023.\\n[34] W. Chen, H. Hu, Y. Li, N. Ruiz, X. Jia, M.-W. Chang, W.W. Cohen, Subject-\\ndriven text-to-image generation via apprenticeship learning, 2023, URL: https:\\n//arxiv.org/abs/2304.00186, arXiv:2304.00186.\\n[35] N. Kumari, B. Zhang, R. Zhang, E. Shechtman, J.-Y. Zhu, Multi-concept \\ncustomization of text-to-image diffusion, in: Proceedings of the IEEE/CVF \\nConference on Computer Vision and Pattern Recognition, 2023, pp. 1931â€“1941.\\n[36] Y. Alaluf, E. Richardson, G. Metzer, D. Cohen-Or, A neural space-time represen-\\ntation for text-to-image personalization, 2023, URL: https://arxiv.org/abs/2305.\\n15391, arXiv:2305.15391.\\n[37] G. Yuan, X. Cun, Y. Zhang, M. Li, C. Qi, X. Wang, Y. Shan, H. Zheng, Inserting \\nanybody in diffusion models via celeb basis, 2023, URL: https://arxiv.org/abs/\\n2306.00926, arXiv:2306.00926.\\n[38] L. Pang, J. Yin, H. Xie, Q. Wang, Q. Li, X. Mao, Cross initialization for \\npersonalized text-to-image generation, 2023, URL: https://arxiv.org/abs/2312.\\n15905, arXiv:2312.15905.\\n[39] R. Liu, B. Ma, W. Zhang, Z. Hu, C. Fan, T. Lv, Y. Ding, X. Cheng, Towards \\na simultaneous and granular identity-expression control in personalized face \\ngeneration, 2024, URL: https://arxiv.org/abs/2401.01207, arXiv:2401.01207.\\n[40] L. Jiang, R. Li, Z. Zhang, S. Fang, C. Ma, EmojiDiff: Advanced facial expression \\ncontrol with high identity preservation in portrait generation, 2024, URL: https:\\n//arxiv.org/abs/2412.01254, arXiv:2412.01254.\\n[41] X. Li, X. Hou, C.C. Loy, When stylegan meets stable diffusion: a w+ adapter for \\npersonalized image generation, in: Proceedings of the IEEE/CVF Conference on \\nComputer Vision and Pattern Recognition, 2024, pp. 2187â€“2196.\\n[42] Y. Wei, Z. Ji, J. Bai, H. Zhang, L. Zhang, W. Zuo, MasterWeaver: Taming \\neditability and face identity for personalized text-to-image generation, 2024, \\nURL: https://arxiv.org/abs/2405.05806, arXiv:2405.05806.\\n[43] M. Arar, A. Voynov, A. Hertz, O. Avrahami, S. Fruchter, Y. Pritch, D. Cohen-Or, \\nA. Shamir, PALP: Prompt aligned personalization of text-to-image models, 2024, \\nURL: https://arxiv.org/abs/2401.06105, arXiv:2401.06105.\\n[44] D. Chae, N. Park, J. Kim, K. Lee, InstructBooth: Instruction-following personalized \\ntext-to-image generation, 2024, URL: https://arxiv.org/abs/2312.03011arXiv:\\n2312.03011.\\n[45] L. Chen, M. Zhao, Y. Liu, M. Ding, Y. Song, S. Wang, X. Wang, H. Yang, J. \\nLiu, K. Du, M. Zheng, PhotoVerse: Tuning-free image customization with text-\\nto-image diffusion models, 2023, URL: https://arxiv.org/abs/2309.05793arXiv:\\n2309.05793.\\n[46] Q. Wu, Y. Liu, H. Zhao, A. Kale, T. Bui, T. Yu, Z. Lin, Y. Zhang, S. Chang, \\nUncovering the disentanglement capability in text-to-image diffusion models, \\nin: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern \\nRecognition, 2023, pp. 1900â€“1910.\\n[47] N. Otberdout, M. Daoudi, A. Kacem, L. Ballihi, S. Berretti, Dynamic facial expres-\\nsion generation on hilbert hypersphere with conditional wasserstein generative \\nadversarial nets, IEEE Trans. Pattern Anal. Mach. Intell. 44 (2) (2020) 848â€“863.\\n[48] Z. Fang, Z. Liu, T. Liu, C.-C. Hung, J. Xiao, G. Feng, Facial expression GAN for \\nvoice-driven face generation, Vis. Comput. (2022) 1â€“14.\\n[49] H. Tang, N. Sebe, Facial expression translation using landmark guided GANs, \\nIEEE Trans. Affect. Comput. 13 (4) (2022) 1986â€“1997, http://dx.doi.org/10.\\n1109/TAFFC.2022.3207007.\\n[50] N. Otberdout, C. Ferrari, M. Daoudi, S. Berretti, A. Del Bimbo, Sparse \\nto dense dynamic 3d facial expression generation, in: Proceedings of the \\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. \\n20385â€“20394.\\n[51] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. \\nSutskever, M. Chen, GLIDE: Towards photorealistic image generation and editing \\nwith text-guided diffusion models, 2022, URL: https://arxiv.org/abs/2112.10741, \\narXiv:2112.10741.\\n[52] J. Deng, J. Guo, N. Xue, S. Zafeiriou, Arcface: Additive angular margin loss for \\ndeep face recognition, in: Proceedings of the IEEE/CVF Conference on Computer \\nVision and Pattern Recognition, 2019, pp. 4690â€“4699.\\n[53] F. Schroff, D. Kalenichenko, J. Philbin, Facenet: A unified embedding for face \\nrecognition and clustering, in: Proceedings of the IEEE Conference on Computer \\nVision and Pattern Recognition, 2015, pp. 815â€“823.\\n[54] D.E. King, Dlib-ml: A machine learning toolkit, J. Mach. Learn. Res. 10 (2009) \\n1755â€“1758.\\n[55] W. Zhang, X. Ji, K. Chen, Y. Ding, C. Fan, Learning a facial expression embedding \\ndisentangled from identity, in: Proceedings of the IEEE/CVF Conference on \\nComputer Vision and Pattern Recognition, CVPR, 2021, pp. 6759â€“6768.\\n[56] L. GarcÃ­a, J. Schmidhuber, Training generative models from scratch: Pitfalls and \\nopportunities, IEEE Trans. Neural Netw. Learn. Syst. 31 (8) (2020) 2541â€“2552, \\nhttp://dx.doi.org/10.1109/TNNLS.2020.3012345.\\n[57] A. Doucet, N.d. Freitas, N. Gordon (Eds.), Sequential Monte Carlo Methods in \\nPractice, Springer, 2001.\\n[58] C. Jarzynski, Equilibrium free-energy differences from nonequilibrium mea-\\nsurements: A master-equation approach, Phys. Rev. E 56 (5) (1997) \\n5018.\\n[59] Y. Song, S. Ermon, Generative modeling by estimating gradients of the data \\ndistribution, Adv. Neural Inf. Process. Syst. 32 (2019).\\n[60] Y. Zhu, Z. Li, T. Wang, M. He, C. Yao, Conditional text image generation with \\ndiffusion models, in: Proceedings of the IEEE/CVF Conference on Computer \\nVision and Pattern Recognition, CVPR, 2023, pp. 14235â€“14245.\\n[61] Z. Huang, K.C. Chan, Y. Jiang, Z. Liu, Collaborative diffusion for multi-modal \\nface generation and editing, in: Proceedings of the IEEE/CVF Conference on \\nComputer Vision and Pattern Recognition, 2023, pp. 6080â€“6090.\\n[62] X. Chen, M. Mihajlovic, S. Wang, S. Prokudin, S. Tang, Morphable diffusion: \\n3D-consistent diffusion for single-image avatar creation, in: Proceedings of the \\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. \\n10359â€“10370.\\n[63] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for biomedi-\\ncal image segmentation, in: Medical Image Computing and Computer-Assisted \\nInterventionâ€“MICCAI 2015: 18th International Conference, Munich, Germany, \\nOctober 5-9, 2015, Proceedings, Part III 18, Springer, 2015, pp. 234â€“241.\\n[64] M.D. Zeiler, R. Fergus, Visualizing and understanding convolutional networks, in: \\nEuropean Conference on Computer Vision, ECCV, Springer, 2014, pp. 818â€“833.\\n[65] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: \\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, \\nCVPR, IEEE, 2016, pp. 770â€“778.\\n[66] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, Å. Kaiser, \\nI. Polosukhin, Attention is all you need, in: Advances in Neural Information \\nProcessing Systems, 2017, pp. 5998â€“6008.\\n[67] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale \\nimage recognition, 2015, URL: https://arxiv.org/abs/1409.1556, arXiv:1409.\\n1556.\\n[68] D. Rolnick, A. Ahuja, J. Schwarz, T. Lillicrap, G. Wayne, Experience replay for \\ncontinual learning, Adv. Neural Inf. Process. Syst. 32 (2019).\\n[69] J. Song, C. Meng, S. Ermon, Denoising diffusion implicit models, 2020.\\n[70] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu, M. Tan, X. \\nWang, W. Liu, B. Xiao, Deep high-resolution representation learning for visual \\nrecognition, 2020, arXiv:1908.07919.\\n[71] Q. Cao, L. Shen, W. Xie, O.M. Parkhi, A. Zisserman, VGGFace2: A dataset \\nfor recognising faces across pose and age, in: 2018 13th IEEE International \\nConference on Automatic Face & Gesture Recognition (FG 2018), IEEE, 2018, \\npp. 67â€“74.\\n[72] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, S. Hochreiter, Gans trained \\nby a two time-scale update rule converge to a local nash equilibrium, Adv. Neural \\nInf. Process. Syst. 30 (2017).\\n[73] A. Radford, J.W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, \\nA. Askell, P. Mishkin, J. Clark, et al., Learning transferable visual models from \\nnatural language supervision, in: International Conference on Machine Learning, \\nPMLR, 2021, pp. 8748â€“8763.\\n[74] F. Carlsson, P. Eisen, F. Rekathati, M. Sahlgren, Cross-lingual and multilingual \\nclip, in: Proceedings of the Thirteenth Language Resources and Evaluation \\nConference, 2022, pp. 6848â€“6854.\\nInformation Fusion 125 (2026) 103431 \\n17 \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz\n",
    "\n",
    "doc = fitz.open('/home/mlevi/OneDrive/Zotero/Afgan et al. - 2025 - Faceexpr Personalized Facial Expression Generation Via Attention-Focused U-Net Feature Fusion in Di.pdf', filetype=\"pdf\")\n",
    "full_text = \"\"\n",
    "for page_num in range(len(doc)):\n",
    "    page = doc.load_page(page_num)\n",
    "    text = page.get_text()\n",
    "    full_text += text\n",
    "doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2463d881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlevi/Work/research-assistant/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-31 23:55:26,246 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-10-31 23:55:26,331 - INFO - Going to convert document batch...\n",
      "2025-10-31 23:55:26,332 - INFO - Initializing pipeline for StandardPdfPipeline with options hash beef5fd6685ff8fca4714abbf6d083c5\n",
      "2025-10-31 23:55:26,338 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-10-31 23:55:26,341 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-10-31 23:55:26,349 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-10-31 23:55:26,352 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-10-31 23:55:26,353 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-10-31 23:55:26,354 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-10-31 23:55:26,800 - INFO - Accelerator device: 'cuda:0'\n",
      "\u001b[32m[INFO] 2025-10-31 23:55:26,825 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-31 23:55:26,849 [RapidOCR] download_file.py:60: File exists and is valid: /home/mlevi/Work/research-assistant/.venv/lib/python3.10/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-31 23:55:26,852 [RapidOCR] torch.py:54: Using /home/mlevi/Work/research-assistant/.venv/lib/python3.10/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-31 23:55:27,099 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-31 23:55:27,105 [RapidOCR] download_file.py:60: File exists and is valid: /home/mlevi/Work/research-assistant/.venv/lib/python3.10/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-31 23:55:27,105 [RapidOCR] torch.py:54: Using /home/mlevi/Work/research-assistant/.venv/lib/python3.10/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-31 23:55:27,202 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-31 23:55:27,239 [RapidOCR] download_file.py:60: File exists and is valid: /home/mlevi/Work/research-assistant/.venv/lib/python3.10/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-31 23:55:27,240 [RapidOCR] torch.py:54: Using /home/mlevi/Work/research-assistant/.venv/lib/python3.10/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-10-31 23:55:27,492 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-10-31 23:55:27,500 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-10-31 23:55:30,057 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-10-31 23:55:30,922 - INFO - Processing document Afgan et al. - 2025 - Faceexpr Personalized Facial Expression Generation Via Attention-Focused U-Net Feature Fusion in Di.pdf\n"
     ]
    }
   ],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import ThreadedPdfPipelineOptions\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.settings import settings\n",
    "\n",
    "# --- Your main settings to add ---\n",
    "\n",
    "# 1. Set the global page batch size to a small number\n",
    "# The default is 4, try setting it to 1\n",
    "settings.perf.page_batch_size = 1\n",
    "\n",
    "# 2. Configure pipeline-specific batch sizes\n",
    "# We are manually setting them to 1 to force sequential processing\n",
    "pipeline_options = ThreadedPdfPipelineOptions(\n",
    "    ocr_batch_size=1,     # Default is 4\n",
    "    layout_batch_size=1,  # Default is 4\n",
    "    table_batch_size=1    # Default is 4\n",
    ")\n",
    "\n",
    "# 3. Pass these options when you create the DocumentConverter\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Now, process your large file ---\n",
    "source = '/home/mlevi/OneDrive/Zotero/Afgan et al. - 2025 - Faceexpr Personalized Facial Expression Generation Via Attention-Focused U-Net Feature Fusion in Di.pdf'\n",
    "doc = doc_converter.convert(source).document\n",
    "\n",
    "print(doc.export_to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1e6409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def get_similarity(str1, str2):\n",
    "    return SequenceMatcher(None, str1.lower(), str2.lower()).ratio()\n",
    "\n",
    "def find_best_match(title, pdf_list, threshold=0.5):\n",
    "    best_match = None\n",
    "    best_score = 0\n",
    "\n",
    "    for pdf in pdf_list:\n",
    "        pdf_name = pdf.replace('.pdf', '').replace('et al.', '')\n",
    "        similarity = get_similarity(title, pdf_name)\n",
    "\n",
    "        if similarity > best_score:\n",
    "            best_score = similarity\n",
    "            best_match = pdf\n",
    "\n",
    "    if best_score >= threshold:\n",
    "        return best_match, best_score\n",
    "    else:\n",
    "        return None, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6117109b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Match (score: 0.67): Facial expression synthesis based on denoising diffusion pro... â†’ Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf\n",
      "âœ“ Match (score: 1.00): Uncertainty-Aware Semi-Supervised Learning of 3D Face Riggin... â†’ Uncertainty-Aware Semi-Supervised Learning of 3D Face Rigging from Single Image.pdf\n",
      "âœ“ Match (score: 1.00): Talking Face Generation with Expression-Tailored Generative ... â†’ Talking Face Generation with Expression-Tailored Generative Adversarial Network.pdf\n",
      "âœ“ Match (score: 0.99): Synthesis of Facial Expressions in Photographs: Characterist... â†’ Synthesis of Facial Expressions in Photographs Characteristics, Approaches, and Challenges.pdf\n",
      "âœ“ Match (score: 0.88): Reviving Intentional Facial Expressions: an Interface for AL... â†’ Reviving Intentional Facial Expressions an Interface for ALS Patients using Brain Decoding and Imag.pdf\n",
      "âœ“ Match (score: 1.00): Fine-grained Micro-Expression Generation based on Thin-Plate... â†’ Fine-grained Micro-Expression Generation based on Thin-Plate Spline and Relative AU Constraint.pdf\n",
      "âœ“ Match (score: 0.98): FAMGAN: Fine-grained AUs Modulation based Generative Adversa... â†’ FAMGAN Fine-grained AUs Modulation based Generative Adversarial Network for Micro-Expression Genera.pdf\n",
      "âœ“ Match (score: 1.00): Facial Image-to-Video Translation by a Hidden Affine Transfo... â†’ Facial Image-to-Video Translation by a Hidden Affine Transformation.pdf\n",
      "âœ“ Match (score: 0.57): Facial Expression Translation using Cycle Consistent Adversa... â†’ Cai et al. - 2021 - Identity-free facial expression recognition using conditional generative adversarial network.pdf\n",
      "âœ— No match (best score: 0.42): CPoser: An Optimization-after-Parsing Approach for Text-to-P...\n",
      "âœ— No match (best score: 0.42): Age-uniform Feature Learning for Image-based Kinship Verific...\n",
      "âœ— No match (best score: 0.37): Deep face normalization...\n",
      "âœ“ Match (score: 0.83): Warp-guided GANs for single-photo facial animation... â†’ Geng et al. - 2018 - Warp-guided GANs for single-photo facial animation.pdf\n",
      "âœ“ Match (score: 0.98): FAMGAN: Fine-grained AUs Modulation based Generative Adversa... â†’ FAMGAN Fine-grained AUs Modulation based Generative Adversarial Network for Micro-Expression Genera.pdf\n",
      "âœ“ Match (score: 1.00): Facial Image-to-Video Translation by a Hidden Affine Transfo... â†’ Facial Image-to-Video Translation by a Hidden Affine Transformation.pdf\n",
      "âœ“ Match (score: 0.57): Facial Expression Translation using Cycle Consistent Adversa... â†’ Cai et al. - 2021 - Identity-free facial expression recognition using conditional generative adversarial network.pdf\n",
      "âœ— No match (best score: 0.42): CPoser: An Optimization-after-Parsing Approach for Text-to-P...\n",
      "âœ— No match (best score: 0.42): Age-uniform Feature Learning for Image-based Kinship Verific...\n",
      "âœ— No match (best score: 0.37): Deep face normalization...\n",
      "âœ“ Match (score: 0.83): Warp-guided GANs for single-photo facial animation... â†’ Geng et al. - 2018 - Warp-guided GANs for single-photo facial animation.pdf\n",
      "âœ“ Match (score: 0.85): ReP USP - Detalhe do registro: Synthesis of facial expressio... â†’ Synthesis of Facial Expressions in Photographs Characteristics, Approaches, and Challenges.pdf\n",
      "âœ“ Match (score: 0.85): StarGAN-EgVA: Emotion Guided Continuous Affect Synthesis... â†’ Yu et al. - 2020 - StarGAN-EgVA Emotion Guided Continuous Affect Synthesis.pdf\n",
      "âœ“ Match (score: 0.88): Self-Supervised Emotion Representation Disentanglement for S... â†’ Xu et al. - 2024 - Self-Supervised Emotion Representation Disentanglement for Speech-Preserving Facial Expression Manip.pdf\n",
      "âœ“ Match (score: 0.89): LGA-GAN: landmarks guided attentive generative adversarial n... â†’ Zhu and Huang - 2022 - LGA-GAN landmarks guided attentive generative adversarial network for facial expression manipulatio.pdf\n",
      "âœ“ Match (score: 0.83): Learning Speech-driven 3D Conversational Gestures from Video... â†’ Habibie et al. - 2021 - Learning Speech-driven 3D Conversational Gestures from Video.pdf\n",
      "âœ“ Match (score: 0.97): \"Just To See You Smile\": SMILEY, a Voice-Guided GUY GAN... â†’ Just To See You Smile SMILEY, a Voice-Guided GUY GAN.pdf\n",
      "âœ“ Match (score: 0.84): InterAct: A Large-Scale Dataset of Dynamic, Expressive and I... â†’ Ho et al. - 2025 - InterAct A Large-Scale Dataset of Dynamic, Expressive and Interactive Activities between Two People.pdf\n",
      "âœ“ Match (score: 0.85): ReP USP - Detalhe do registro: Synthesis of facial expressio... â†’ Synthesis of Facial Expressions in Photographs Characteristics, Approaches, and Challenges.pdf\n",
      "âœ“ Match (score: 0.85): StarGAN-EgVA: Emotion Guided Continuous Affect Synthesis... â†’ Yu et al. - 2020 - StarGAN-EgVA Emotion Guided Continuous Affect Synthesis.pdf\n",
      "âœ“ Match (score: 0.88): Self-Supervised Emotion Representation Disentanglement for S... â†’ Xu et al. - 2024 - Self-Supervised Emotion Representation Disentanglement for Speech-Preserving Facial Expression Manip.pdf\n",
      "âœ“ Match (score: 0.89): LGA-GAN: landmarks guided attentive generative adversarial n... â†’ Zhu and Huang - 2022 - LGA-GAN landmarks guided attentive generative adversarial network for facial expression manipulatio.pdf\n",
      "âœ“ Match (score: 0.83): Learning Speech-driven 3D Conversational Gestures from Video... â†’ Habibie et al. - 2021 - Learning Speech-driven 3D Conversational Gestures from Video.pdf\n",
      "âœ“ Match (score: 0.97): \"Just To See You Smile\": SMILEY, a Voice-Guided GUY GAN... â†’ Just To See You Smile SMILEY, a Voice-Guided GUY GAN.pdf\n",
      "âœ“ Match (score: 0.84): InterAct: A Large-Scale Dataset of Dynamic, Expressive and I... â†’ Ho et al. - 2025 - InterAct A Large-Scale Dataset of Dynamic, Expressive and Interactive Activities between Two People.pdf\n",
      "âœ“ Match (score: 0.89): GiGAN: Gate in GAN, could gate mechanism filter the features... â†’ Nie et al. - 2021 - GiGAN Gate in GAN, could gate mechanism filter the features in image-to-image translation.pdf\n",
      "âœ“ Match (score: 1.00): Generating Head Animation for a Back-Projected Human-Like Ro... â†’ Generating Head Animation for a Back-Projected Human-Like Robot Head.pdf\n",
      "âœ“ Match (score: 0.90): Forgery Detection by Weighted Complementarity between Signif... â†’ Xiao et al. - 2023 - Forgery Detection by Weighted Complementarity between Significant Invariance and Detail Enhancement.pdf\n",
      "âœ“ Match (score: 0.69): Facial expression synthesis by u-net conditional generative ... â†’ Cai et al. - 2021 - Identity-free facial expression recognition using conditional generative adversarial network.pdf\n",
      "âœ“ Match (score: 0.87): Facial Expression Generation Based on Multi-Scale Mixed Atte... â†’ Liu et al. - 2024 - Facial expression generation based on multi-scale mixed attention.pdf\n",
      "âœ“ Match (score: 0.88): Face Reenactment with Diffusion Model and Its Application to... â†’ Iuchi et al. - 2023 - Face reenactment with diffusion model and its application to video compression.pdf\n",
      "âœ“ Match (score: 0.54): ExprADA: Adversarial domain adaptation for facial expression... â†’ Kuang et al. - 2019 - Multi-expression generative adversarial networks for facial expression synthesis.pdf\n",
      "âœ“ Match (score: 0.89): GiGAN: Gate in GAN, could gate mechanism filter the features... â†’ Nie et al. - 2021 - GiGAN Gate in GAN, could gate mechanism filter the features in image-to-image translation.pdf\n",
      "âœ“ Match (score: 1.00): Generating Head Animation for a Back-Projected Human-Like Ro... â†’ Generating Head Animation for a Back-Projected Human-Like Robot Head.pdf\n",
      "âœ“ Match (score: 0.90): Forgery Detection by Weighted Complementarity between Signif... â†’ Xiao et al. - 2023 - Forgery Detection by Weighted Complementarity between Significant Invariance and Detail Enhancement.pdf\n",
      "âœ“ Match (score: 0.69): Facial expression synthesis by u-net conditional generative ... â†’ Cai et al. - 2021 - Identity-free facial expression recognition using conditional generative adversarial network.pdf\n",
      "âœ“ Match (score: 0.87): Facial Expression Generation Based on Multi-Scale Mixed Atte... â†’ Liu et al. - 2024 - Facial expression generation based on multi-scale mixed attention.pdf\n",
      "âœ“ Match (score: 0.88): Face Reenactment with Diffusion Model and Its Application to... â†’ Iuchi et al. - 2023 - Face reenactment with diffusion model and its application to video compression.pdf\n",
      "âœ“ Match (score: 0.54): ExprADA: Adversarial domain adaptation for facial expression... â†’ Kuang et al. - 2019 - Multi-expression generative adversarial networks for facial expression synthesis.pdf\n",
      "âœ“ Match (score: 0.79): Enhancing Facial Expression Synthesis through GAN with Multi... â†’ Nimitha et al. - 2023 - Enhancing facial expression synthesis through GAN with multi-scale dilated feature extraction and ed.pdf\n",
      "âœ“ Match (score: 0.86): Double Encoder Conditional GAN for Facial Expression Synthes... â†’ Chen et al. - 2018 - Double encoder conditional GAN for facial expression synthesis.pdf\n",
      "âœ“ Match (score: 0.76): Does the Input Image Spatial Resolution Generate Different S... â†’ Testa et al. - 2023 - Does the input image spatial resolution generate different synthetic images A comparative study of.pdf\n",
      "âœ“ Match (score: 0.50): Conditional expression synthesis with face parsing transform... â†’ Zhai et al. - 2023 - Facial expression synthesis and recognition with pre-trained StyleGAN.pdf\n",
      "âœ“ Match (score: 0.89): AUD:AU-based Diffusion model for Facial Expression Synthesis... â†’ Xu et al. - 2024 - AUDAU-based diffusion model for facial expression synthesis from a single image.pdf\n",
      "âœ“ Match (score: 0.81): Attention Based Facial Expression Manipulation... â†’ Wang et al. - 2021 - Attention based facial expression manipulation.pdf\n",
      "âœ“ Match (score: 0.70): AnimateMe: 4D Facial Expressions viaÂ Diffusion Models... â†’ Zou et al. - 2024 - 4D facial expression diffusion model.pdf\n",
      "âœ“ Match (score: 0.83): Action Unit Based Smiling Face Generation Method... â†’ Gao et al. - 2023 - Action unit based smiling face generation method.pdf\n",
      "âœ“ Match (score: 0.56): Faceexpr: Personalized Facial Expression Generation Via Atte... â†’ Shu and Koike - 2025 - Pioneering facial expression generation from sEMG signals with diffusion models.pdf\n",
      "âœ“ Match (score: 0.79): Enhancing Facial Expression Synthesis through GAN with Multi... â†’ Nimitha et al. - 2023 - Enhancing facial expression synthesis through GAN with multi-scale dilated feature extraction and ed.pdf\n",
      "âœ“ Match (score: 0.86): Double Encoder Conditional GAN for Facial Expression Synthes... â†’ Chen et al. - 2018 - Double encoder conditional GAN for facial expression synthesis.pdf\n",
      "âœ“ Match (score: 0.76): Does the Input Image Spatial Resolution Generate Different S... â†’ Testa et al. - 2023 - Does the input image spatial resolution generate different synthetic images A comparative study of.pdf\n",
      "âœ“ Match (score: 0.50): Conditional expression synthesis with face parsing transform... â†’ Zhai et al. - 2023 - Facial expression synthesis and recognition with pre-trained StyleGAN.pdf\n",
      "âœ“ Match (score: 0.89): AUD:AU-based Diffusion model for Facial Expression Synthesis... â†’ Xu et al. - 2024 - AUDAU-based diffusion model for facial expression synthesis from a single image.pdf\n",
      "âœ“ Match (score: 0.81): Attention Based Facial Expression Manipulation... â†’ Wang et al. - 2021 - Attention based facial expression manipulation.pdf\n",
      "âœ“ Match (score: 0.70): AnimateMe: 4D Facial Expressions viaÂ Diffusion Models... â†’ Zou et al. - 2024 - 4D facial expression diffusion model.pdf\n",
      "âœ“ Match (score: 0.83): Action Unit Based Smiling Face Generation Method... â†’ Gao et al. - 2023 - Action unit based smiling face generation method.pdf\n",
      "âœ“ Match (score: 0.56): Faceexpr: Personalized Facial Expression Generation Via Atte... â†’ Shu and Koike - 2025 - Pioneering facial expression generation from sEMG signals with diffusion models.pdf\n",
      "âœ“ Match (score: 0.54): A highly naturalistic facial expression generation method wi... â†’ Shu and Koike - 2025 - Pioneering facial expression generation from sEMG signals with diffusion models.pdf\n",
      "âœ— No match (best score: 0.48): A generative approach for dynamically varying photorealistic...\n",
      "âœ“ Match (score: 0.68): A facial expression synthesis method based on generative adv... â†’ Talking Face Generation with Expression-Tailored Generative Adversarial Network.pdf\n",
      "âœ“ Match (score: 0.84): Geometry guided adversarial facial expression synthesis... â†’ Song et al. - 2018 - Geometry guided adversarial facial expression synthesis.pdf\n",
      "âœ“ Match (score: 0.78): 4D facial expression diffusion model... â†’ Zou et al. - 2024 - 4D facial expression diffusion model.pdf\n",
      "âœ“ Match (score: 0.86): Intuitive facial animation editing based on a generative RNN... â†’ Berson et al. - 2020 - Intuitive facial animation editing based on a generative RNN framework.pdf\n",
      "âœ— No match (best score: 0.47): Facial Expression Analysis and Its Potentials in IoT Systems...\n",
      "âœ“ Match (score: 0.52): Facial Expression Analysis in Parkinsonsâ€™s Disease Using Mac... â†’ Reviving Intentional Facial Expressions an Interface for ALS Patients using Brain Decoding and Imag.pdf\n",
      "âœ— No match (best score: 0.47): Deepfake Detection Using Spatiotemporal Transformer...\n",
      "âœ“ Match (score: 0.54): A highly naturalistic facial expression generation method wi... â†’ Shu and Koike - 2025 - Pioneering facial expression generation from sEMG signals with diffusion models.pdf\n",
      "âœ— No match (best score: 0.48): A generative approach for dynamically varying photorealistic...\n",
      "âœ“ Match (score: 0.68): A facial expression synthesis method based on generative adv... â†’ Talking Face Generation with Expression-Tailored Generative Adversarial Network.pdf\n",
      "âœ“ Match (score: 0.84): Geometry guided adversarial facial expression synthesis... â†’ Song et al. - 2018 - Geometry guided adversarial facial expression synthesis.pdf\n",
      "âœ“ Match (score: 0.78): 4D facial expression diffusion model... â†’ Zou et al. - 2024 - 4D facial expression diffusion model.pdf\n",
      "âœ“ Match (score: 0.86): Intuitive facial animation editing based on a generative RNN... â†’ Berson et al. - 2020 - Intuitive facial animation editing based on a generative RNN framework.pdf\n",
      "âœ— No match (best score: 0.47): Facial Expression Analysis and Its Potentials in IoT Systems...\n",
      "âœ“ Match (score: 0.52): Facial Expression Analysis in Parkinsonsâ€™s Disease Using Mac... â†’ Reviving Intentional Facial Expressions an Interface for ALS Patients using Brain Decoding and Imag.pdf\n",
      "âœ— No match (best score: 0.47): Deepfake Detection Using Spatiotemporal Transformer...\n",
      "âœ— No match (best score: 0.42): Detection of AI-Manipulated Fake Faces via Mining Generalize...\n",
      "âœ“ Match (score: 0.77): SMART-DREAM: To Condition or Not to Condition; A Study on th... â†’ Galland et al. - 2025 - SMART-DREAM To Condition or Not to Condition; A Study on the Impact of LLM Conditioning on Motivati.pdf\n",
      "âœ“ Match (score: 0.83): Human Latent Metrics: Perceptual and Cognitive Response Corr... â†’ Shimizu et al. - 2022 - Human Latent Metrics Perceptual and Cognitive Response Correlates to Distance in GAN Latent Space f.pdf\n",
      "âœ“ Match (score: 0.83): Generative Adversarial Networks for Face Generation: A Surve... â†’ Kammoun et al. - 2022 - Generative Adversarial Networks for Face Generation A Survey.pdf\n",
      "âœ“ Match (score: 0.53): Data-driven Communicative Behaviour Generation: A Survey... â†’ Kammoun et al. - 2022 - Generative Adversarial Networks for Face Generation A Survey.pdf\n",
      "âœ“ Match (score: 0.88): FC-4DFS: Frequency-controlled Flexible 4D Facial Expression ... â†’ Lu et al. - 2024 - FC-4DFS Frequency-controlled Flexible 4D Facial Expression Synthesizing.pdf\n",
      "âœ“ Match (score: 0.63): Cycle In Cycle Generative Adversarial Networks for Keypoint-... â†’ Kammoun et al. - 2022 - Generative Adversarial Networks for Face Generation A Survey.pdf\n",
      "âœ— No match (best score: 0.42): Detection of AI-Manipulated Fake Faces via Mining Generalize...\n",
      "âœ“ Match (score: 0.77): SMART-DREAM: To Condition or Not to Condition; A Study on th... â†’ Galland et al. - 2025 - SMART-DREAM To Condition or Not to Condition; A Study on the Impact of LLM Conditioning on Motivati.pdf\n",
      "âœ“ Match (score: 0.83): Human Latent Metrics: Perceptual and Cognitive Response Corr... â†’ Shimizu et al. - 2022 - Human Latent Metrics Perceptual and Cognitive Response Correlates to Distance in GAN Latent Space f.pdf\n",
      "âœ“ Match (score: 0.83): Generative Adversarial Networks for Face Generation: A Surve... â†’ Kammoun et al. - 2022 - Generative Adversarial Networks for Face Generation A Survey.pdf\n",
      "âœ“ Match (score: 0.53): Data-driven Communicative Behaviour Generation: A Survey... â†’ Kammoun et al. - 2022 - Generative Adversarial Networks for Face Generation A Survey.pdf\n",
      "âœ“ Match (score: 0.88): FC-4DFS: Frequency-controlled Flexible 4D Facial Expression ... â†’ Lu et al. - 2024 - FC-4DFS Frequency-controlled Flexible 4D Facial Expression Synthesizing.pdf\n",
      "âœ“ Match (score: 0.63): Cycle In Cycle Generative Adversarial Networks for Keypoint-... â†’ Kammoun et al. - 2022 - Generative Adversarial Networks for Face Generation A Survey.pdf\n",
      "âœ— No match (best score: 0.45): Facial Expression Modeling and Synthesis for Patient Simulat...\n",
      "âœ“ Match (score: 0.89): Follow-Your-Emoji: Fine-Controllable and Expressive Freestyl... â†’ Ma et al. - 2024 - Follow-Your-Emoji Fine-Controllable and Expressive Freestyle Portrait Animation.pdf\n",
      "âœ“ Match (score: 0.88): Greta 2.0: Social Interactive Agent system, optimized for ne... â†’ Saga et al. - 2025 - Greta 2.0 Social Interactive Agent system, optimized for neural network integration.pdf\n",
      "âœ“ Match (score: 0.90): U-Net Conditional GANs for Photo-Realistic and Identity-Pres... â†’ Wang et al. - 2019 - U-Net Conditional GANs for Photo-Realistic and Identity-Preserving Facial Expression Synthesis.pdf\n",
      "âœ“ Match (score: 0.85): WTV-GANimation: High quality facial expression synthesis gen... â†’ Zhao and Zhang - 2022 - WTV-GANimation High quality facial expression synthesis generation model.pdf\n",
      "âœ“ Match (score: 0.88): Unpaired images based generator architecture for facial expr... â†’ Zhang et al. - 2019 - Unpaired images based generator architecture for facial expression recognition.pdf\n",
      "âœ“ Match (score: 0.83): The technology of generating facial expressions for film and... â†’ Zhang and Qian - 2024 - The technology of generating facial expressions for film and television characters based on deep lea.pdf\n",
      "âœ— No match (best score: 0.45): Facial Expression Modeling and Synthesis for Patient Simulat...\n",
      "âœ“ Match (score: 0.89): Follow-Your-Emoji: Fine-Controllable and Expressive Freestyl... â†’ Ma et al. - 2024 - Follow-Your-Emoji Fine-Controllable and Expressive Freestyle Portrait Animation.pdf\n",
      "âœ“ Match (score: 0.88): Greta 2.0: Social Interactive Agent system, optimized for ne... â†’ Saga et al. - 2025 - Greta 2.0 Social Interactive Agent system, optimized for neural network integration.pdf\n",
      "âœ“ Match (score: 0.90): U-Net Conditional GANs for Photo-Realistic and Identity-Pres... â†’ Wang et al. - 2019 - U-Net Conditional GANs for Photo-Realistic and Identity-Preserving Facial Expression Synthesis.pdf\n",
      "âœ“ Match (score: 0.85): WTV-GANimation: High quality facial expression synthesis gen... â†’ Zhao and Zhang - 2022 - WTV-GANimation High quality facial expression synthesis generation model.pdf\n",
      "âœ“ Match (score: 0.88): Unpaired images based generator architecture for facial expr... â†’ Zhang et al. - 2019 - Unpaired images based generator architecture for facial expression recognition.pdf\n",
      "âœ“ Match (score: 0.83): The technology of generating facial expressions for film and... â†’ Zhang and Qian - 2024 - The technology of generating facial expressions for film and television characters based on deep lea.pdf\n",
      "âœ“ Match (score: 0.87): Synthetic expressions are better than real for learning to d... â†’ Niinuma et al. - 2021 - Synthetic expressions are better than real for learning to detect facial actions.pdf\n",
      "âœ“ Match (score: 0.81): Sparse to dense dynamic 3D facial expression generation... â†’ Otberdout et al. - 2022 - Sparse to dense dynamic 3D facial expression generation.pdf\n",
      "âœ“ Match (score: 0.82): Revitalizing nash equilibrium in gans for human face image g... â†’ Khodabandelou et al. - 2024 - Revitalizing nash equilibrium in gans for human face image generation.pdf\n",
      "âœ“ Match (score: 0.76): Pixel-based facial expression synthesis... â†’ Akram and Khan - 2020 - Pixel-based facial expression synthesis.pdf\n",
      "âœ“ Match (score: 0.87): Pioneering facial expression generation from sEMG signals wi... â†’ Shu and Koike - 2025 - Pioneering facial expression generation from sEMG signals with diffusion models.pdf\n",
      "âœ“ Match (score: 0.79): Music recommendation system based on facial emotion analysis... â†’ Priyadharshini et al. - 2025 - Music recommendation system based on facial emotion analysis.pdf\n",
      "âœ“ Match (score: 0.88): Multi-expression generative adversarial networks for facial ... â†’ Kuang et al. - 2019 - Multi-expression generative adversarial networks for facial expression synthesis.pdf\n",
      "âœ“ Match (score: 0.90): Local and global perception generative adversarial network f... â†’ Xia et al. - 2022 - Local and global perception generative adversarial network for facial expression synthesis.pdf\n",
      "âœ“ Match (score: 0.82): Motion-oriented diffusion models for facial expression synth... â†’ Bouzid and Ballihi - 2024 - Motion-oriented diffusion models for facial expression synthesis.pdf\n",
      "âœ“ Match (score: 0.87): Synthetic expressions are better than real for learning to d... â†’ Niinuma et al. - 2021 - Synthetic expressions are better than real for learning to detect facial actions.pdf\n",
      "âœ“ Match (score: 0.81): Sparse to dense dynamic 3D facial expression generation... â†’ Otberdout et al. - 2022 - Sparse to dense dynamic 3D facial expression generation.pdf\n",
      "âœ“ Match (score: 0.82): Revitalizing nash equilibrium in gans for human face image g... â†’ Khodabandelou et al. - 2024 - Revitalizing nash equilibrium in gans for human face image generation.pdf\n",
      "âœ“ Match (score: 0.76): Pixel-based facial expression synthesis... â†’ Akram and Khan - 2020 - Pixel-based facial expression synthesis.pdf\n",
      "âœ“ Match (score: 0.87): Pioneering facial expression generation from sEMG signals wi... â†’ Shu and Koike - 2025 - Pioneering facial expression generation from sEMG signals with diffusion models.pdf\n",
      "âœ“ Match (score: 0.79): Music recommendation system based on facial emotion analysis... â†’ Priyadharshini et al. - 2025 - Music recommendation system based on facial emotion analysis.pdf\n",
      "âœ“ Match (score: 0.88): Multi-expression generative adversarial networks for facial ... â†’ Kuang et al. - 2019 - Multi-expression generative adversarial networks for facial expression synthesis.pdf\n",
      "âœ“ Match (score: 0.90): Local and global perception generative adversarial network f... â†’ Xia et al. - 2022 - Local and global perception generative adversarial network for facial expression synthesis.pdf\n",
      "âœ“ Match (score: 0.82): Motion-oriented diffusion models for facial expression synth... â†’ Bouzid and Ballihi - 2024 - Motion-oriented diffusion models for facial expression synthesis.pdf\n",
      "âœ“ Match (score: 0.81): Generative adversarial networks in human emotion synthesis: ... â†’ Hajarolasvadi et al. - 2020 - Generative adversarial networks in human emotion synthesis a review.pdf\n",
      "âœ“ Match (score: 0.87): Joint deep learning of facial expression synthesis and recog... â†’ Yan et al. - 2020 - Joint deep learning of facial expression synthesis and recognition.pdf\n",
      "âœ“ Match (score: 0.90): Identity-free facial expression recognition using conditiona... â†’ Cai et al. - 2021 - Identity-free facial expression recognition using conditional generative adversarial network.pdf\n",
      "âœ“ Match (score: 0.87): Generating multiple 4D expression transitions by learning fa... â†’ Otberdout et al. - 2024 - Generating multiple 4D expression transitions by learning face landmark trajectories.pdf\n",
      "âœ“ Match (score: 0.86): Fine-grained expression manipulation via structured latent s... â†’ Tang et al. - 2020 - Fine-grained expression manipulation via structured latent space.pdf\n",
      "âœ“ Match (score: 0.83): Facial expression translation using landmark guided gans... â†’ Tang and Sebe - 2022 - Facial expression translation using landmark guided gans.pdf\n",
      "âœ“ Match (score: 0.87): Facial expression synthesis and recognition with pre-trained... â†’ Zhai et al. - 2023 - Facial expression synthesis and recognition with pre-trained StyleGAN.pdf\n",
      "âœ“ Match (score: 0.89): Facial age and expression synthesis using ordinal ranking ad... â†’ Sun et al. - 2020 - Facial age and expression synthesis using ordinal ranking adversarial networks.pdf\n",
      "âœ“ Match (score: 0.88): Expression conditional gan for facial expression-to-expressi... â†’ Tang et al. - 2019 - Expression conditional gan for facial expression-to-expression translation.pdf\n",
      "âœ“ Match (score: 0.81): Generative adversarial networks in human emotion synthesis: ... â†’ Hajarolasvadi et al. - 2020 - Generative adversarial networks in human emotion synthesis a review.pdf\n",
      "âœ“ Match (score: 0.87): Joint deep learning of facial expression synthesis and recog... â†’ Yan et al. - 2020 - Joint deep learning of facial expression synthesis and recognition.pdf\n",
      "âœ“ Match (score: 0.90): Identity-free facial expression recognition using conditiona... â†’ Cai et al. - 2021 - Identity-free facial expression recognition using conditional generative adversarial network.pdf\n",
      "âœ“ Match (score: 0.87): Generating multiple 4D expression transitions by learning fa... â†’ Otberdout et al. - 2024 - Generating multiple 4D expression transitions by learning face landmark trajectories.pdf\n",
      "âœ“ Match (score: 0.86): Fine-grained expression manipulation via structured latent s... â†’ Tang et al. - 2020 - Fine-grained expression manipulation via structured latent space.pdf\n",
      "âœ“ Match (score: 0.83): Facial expression translation using landmark guided gans... â†’ Tang and Sebe - 2022 - Facial expression translation using landmark guided gans.pdf\n",
      "âœ“ Match (score: 0.87): Facial expression synthesis and recognition with pre-trained... â†’ Zhai et al. - 2023 - Facial expression synthesis and recognition with pre-trained StyleGAN.pdf\n",
      "âœ“ Match (score: 0.89): Facial age and expression synthesis using ordinal ranking ad... â†’ Sun et al. - 2020 - Facial age and expression synthesis using ordinal ranking adversarial networks.pdf\n",
      "âœ“ Match (score: 0.88): Expression conditional gan for facial expression-to-expressi... â†’ Tang et al. - 2019 - Expression conditional gan for facial expression-to-expression translation.pdf\n",
      "âœ“ Match (score: 0.87): Dynamic facial expression synthesis driven by deformable sem... â†’ Gong et al. - 2018 - Dynamic facial expression synthesis driven by deformable semantic parts.pdf\n",
      "âœ“ Match (score: 0.82): Dynamic facial expression generation on hilbert hypersphere ... â†’ Otberdout et al. - 2022 - Dynamic facial expression generation on hilbert hypersphere with conditional wasserstein generative.pdf\n",
      "âœ“ Match (score: 0.87): 3D dense geometry-guided facial expression synthesis by adve... â†’ Bodur et al. - 2021 - 3D dense geometry-guided facial expression synthesis by adversarial learning.pdf\n",
      "âœ“ Match (score: 0.86): Simulation of Facial Palsy using Conditional Generative Adve... â†’ Yaotome et al. - 2019 - Simulation of Facial Palsy using Conditional Generative Adversarial Networks.pdf\n",
      "âœ“ Match (score: 0.87): Multiple Facial Expressions Synthesis Driven by Editable Lin... â†’ Liu et al. - 2020 - Multiple Facial Expressions Synthesis Driven by Editable Line Maps.pdf\n",
      "âœ“ Match (score: 0.72): SynExpression: A Diffusion-Based Framework for Controllable ... â†’ Sayyafzadeh et al. - 2025 - SynExpression A Diffusion-Based Framework for Controllable Facial Expression Synthesis and Emotion.pdf\n",
      "âœ“ Match (score: 0.57): DiffAtten-GAN: A Local Attention-Enhanced Diffusion-GAN for ... â†’ Bouzid and Ballihi - 2024 - Motion-oriented diffusion models for facial expression synthesis.pdf\n",
      "âœ“ Match (score: 0.87): Dynamic facial expression synthesis driven by deformable sem... â†’ Gong et al. - 2018 - Dynamic facial expression synthesis driven by deformable semantic parts.pdf\n",
      "âœ“ Match (score: 0.82): Dynamic facial expression generation on hilbert hypersphere ... â†’ Otberdout et al. - 2022 - Dynamic facial expression generation on hilbert hypersphere with conditional wasserstein generative.pdf\n",
      "âœ“ Match (score: 0.87): 3D dense geometry-guided facial expression synthesis by adve... â†’ Bodur et al. - 2021 - 3D dense geometry-guided facial expression synthesis by adversarial learning.pdf\n",
      "âœ“ Match (score: 0.86): Simulation of Facial Palsy using Conditional Generative Adve... â†’ Yaotome et al. - 2019 - Simulation of Facial Palsy using Conditional Generative Adversarial Networks.pdf\n",
      "âœ“ Match (score: 0.87): Multiple Facial Expressions Synthesis Driven by Editable Lin... â†’ Liu et al. - 2020 - Multiple Facial Expressions Synthesis Driven by Editable Line Maps.pdf\n",
      "âœ“ Match (score: 0.72): SynExpression: A Diffusion-Based Framework for Controllable ... â†’ Sayyafzadeh et al. - 2025 - SynExpression A Diffusion-Based Framework for Controllable Facial Expression Synthesis and Emotion.pdf\n",
      "âœ“ Match (score: 0.57): DiffAtten-GAN: A Local Attention-Enhanced Diffusion-GAN for ... â†’ Bouzid and Ballihi - 2024 - Motion-oriented diffusion models for facial expression synthesis.pdf\n",
      "âœ“ Match (score: 0.87): US-GAN: on the importance of ultimate skip connection for fa... â†’ Akram and Khan - 2024 - US-GAN on the importance of ultimate skip connection for facial expression synthesis.pdf\n",
      "âœ“ Match (score: 0.85): Facial expression video generation based-on spatio-temporal ... â†’ Bouzid and Ballihi - 2022 - Facial expression video generation based-on spatio-temporal convolutional GAN FEV-GAN.pdf\n",
      "âœ“ Match (score: 0.86): SARGAN: Spatial attention-based residuals for facial express... â†’ Akram and Khan - 2023 - SARGAN Spatial attention-based residuals for facial expression manipulation.pdf\n",
      "âœ“ Match (score: 0.86): Facial expressions generating model reflecting agent's emoti... â†’ Kondo et al. - 2022 - Facial expressions generating model reflecting agent's emotion response using facial landmark residu.pdf\n",
      "âœ“ Match (score: 0.86): Facial expression manipulation for personalized facial actio... â†’ Niinuma et al. - 2022 - Facial expression manipulation for personalized facial action estimation.pdf\n",
      "âœ“ Match (score: 0.83): SliderGAN: Synthesizing expressive face images by sliding 3D... â†’ Ververas and Zafeiriou - 2020 - SliderGAN Synthesizing expressive face images by sliding 3D blendshape parameters.pdf\n",
      "âœ“ Match (score: 0.90): Masked linear regression for learning local receptive fields... â†’ Khan et al. - 2020 - Masked linear regression for learning local receptive fields for facial expression synthesis.pdf\n",
      "âœ“ Match (score: 0.82): GANimation: One-shot anatomically consistent facial animatio... â†’ Pumarola et al. - 2020 - GANimation One-shot anatomically consistent facial animation.pdf\n",
      "âœ“ Match (score: 0.85): Region based adversarial synthesis of facial action units... â†’ Liu et al. - 2020 - Region based adversarial synthesis of facial action units.pdf\n",
      "âœ“ Match (score: 0.87): US-GAN: on the importance of ultimate skip connection for fa... â†’ Akram and Khan - 2024 - US-GAN on the importance of ultimate skip connection for facial expression synthesis.pdf\n",
      "âœ“ Match (score: 0.85): Facial expression video generation based-on spatio-temporal ... â†’ Bouzid and Ballihi - 2022 - Facial expression video generation based-on spatio-temporal convolutional GAN FEV-GAN.pdf\n",
      "âœ“ Match (score: 0.86): SARGAN: Spatial attention-based residuals for facial express... â†’ Akram and Khan - 2023 - SARGAN Spatial attention-based residuals for facial expression manipulation.pdf\n",
      "âœ“ Match (score: 0.86): Facial expressions generating model reflecting agent's emoti... â†’ Kondo et al. - 2022 - Facial expressions generating model reflecting agent's emotion response using facial landmark residu.pdf\n",
      "âœ“ Match (score: 0.86): Facial expression manipulation for personalized facial actio... â†’ Niinuma et al. - 2022 - Facial expression manipulation for personalized facial action estimation.pdf\n",
      "âœ“ Match (score: 0.83): SliderGAN: Synthesizing expressive face images by sliding 3D... â†’ Ververas and Zafeiriou - 2020 - SliderGAN Synthesizing expressive face images by sliding 3D blendshape parameters.pdf\n",
      "âœ“ Match (score: 0.90): Masked linear regression for learning local receptive fields... â†’ Khan et al. - 2020 - Masked linear regression for learning local receptive fields for facial expression synthesis.pdf\n",
      "âœ“ Match (score: 0.82): GANimation: One-shot anatomically consistent facial animatio... â†’ Pumarola et al. - 2020 - GANimation One-shot anatomically consistent facial animation.pdf\n",
      "âœ“ Match (score: 0.85): Region based adversarial synthesis of facial action units... â†’ Liu et al. - 2020 - Region based adversarial synthesis of facial action units.pdf\n",
      "âœ“ Match (score: 0.89): Synthesizing coupled 3D face modalities by trunk-branch gene... â†’ Gecer et al. - 2020 - Synthesizing coupled 3D face modalities by trunk-branch generative adversarial networks.pdf\n",
      "âœ“ Match (score: 0.84): GANimation: Anatomically-aware facial animation from a singl... â†’ Pumarola et al. - 2018 - GANimation Anatomically-aware facial animation from a single image.pdf\n",
      "âœ“ Match (score: 0.88): Generative cooperative net for image generation and data aug... â†’ Xu et al. - 2019 - Generative cooperative net for image generation and data augmentation.pdf\n",
      "âœ“ Match (score: 0.84): Apprgan: Appearance-based GAN for facial expression synthesi... â†’ Peng and Yin - 2019 - Apprgan Appearance-based GAN for facial expression synthesis.pdf\n",
      "âœ“ Match (score: 0.90): Action unit driven facial expression synthesis from a single... â†’ Zhao et al. - 2021 - Action unit driven facial expression synthesis from a single image with patch attentive GAN.pdf\n",
      "âœ“ Match (score: 0.87): Attention-based image-to-video translation for synthesizing ... â†’ Alemayehu et al. - 2023 - Attention-based image-to-video translation for synthesizing facial expression using GAN.pdf\n",
      "âœ“ Match (score: 0.87): Bipartite graph reasoning gans for person pose and facial im... â†’ Tang et al. - 2023 - Bipartite graph reasoning gans for person pose and facial image synthesis.pdf\n",
      "âœ“ Match (score: 0.82): EvoGAN: An evolutionary computation assisted GAN... â†’ Liu et al. - 2022 - EvoGAN An evolutionary computation assisted GAN.pdf\n",
      "âœ“ Match (score: 0.89): Contour wavelet diffusionâ€“a fast and high-quality facial exp... â†’ Xu and Zou - 2024 - Contour wavelet diffusionâ€“a fast and high-quality facial expression generation model.pdf\n",
      "âœ“ Match (score: 0.89): Synthesizing coupled 3D face modalities by trunk-branch gene... â†’ Gecer et al. - 2020 - Synthesizing coupled 3D face modalities by trunk-branch generative adversarial networks.pdf\n",
      "âœ“ Match (score: 0.84): GANimation: Anatomically-aware facial animation from a singl... â†’ Pumarola et al. - 2018 - GANimation Anatomically-aware facial animation from a single image.pdf\n",
      "âœ“ Match (score: 0.88): Generative cooperative net for image generation and data aug... â†’ Xu et al. - 2019 - Generative cooperative net for image generation and data augmentation.pdf\n",
      "âœ“ Match (score: 0.84): Apprgan: Appearance-based GAN for facial expression synthesi... â†’ Peng and Yin - 2019 - Apprgan Appearance-based GAN for facial expression synthesis.pdf\n",
      "âœ“ Match (score: 0.90): Action unit driven facial expression synthesis from a single... â†’ Zhao et al. - 2021 - Action unit driven facial expression synthesis from a single image with patch attentive GAN.pdf\n",
      "âœ“ Match (score: 0.87): Attention-based image-to-video translation for synthesizing ... â†’ Alemayehu et al. - 2023 - Attention-based image-to-video translation for synthesizing facial expression using GAN.pdf\n",
      "âœ“ Match (score: 0.87): Bipartite graph reasoning gans for person pose and facial im... â†’ Tang et al. - 2023 - Bipartite graph reasoning gans for person pose and facial image synthesis.pdf\n",
      "âœ“ Match (score: 0.82): EvoGAN: An evolutionary computation assisted GAN... â†’ Liu et al. - 2022 - EvoGAN An evolutionary computation assisted GAN.pdf\n",
      "âœ“ Match (score: 0.89): Contour wavelet diffusionâ€“a fast and high-quality facial exp... â†’ Xu and Zou - 2024 - Contour wavelet diffusionâ€“a fast and high-quality facial expression generation model.pdf\n",
      "âœ“ Match (score: 0.87): Semantic prior guided fine-grained facial expression manipul... â†’ Xue et al. - 2024 - Semantic prior guided fine-grained facial expression manipulation.pdf\n",
      "âœ“ Match (score: 0.80): Facial expression morphing: enhancing visual fidelity and pr... â†’ Sub-R-Pa et al. - 2024 - Facial expression morphing enhancing visual fidelity and preserving facial details in CycleGAN-base.pdf\n",
      "âœ“ Match (score: 0.93): Artificial intelligence D riven physical simulation and anim... â†’ Wang - 2025 - Artificial intelligence D riven physical simulation and animation generation in computer graphics.pdf\n",
      "âœ“ Match (score: 0.85): Facial expression generation from text with FaceCLIP... â†’ Fu et al. - 2025 - Facial expression generation from text with FaceCLIP.pdf\n",
      "\n",
      "================================================================================\n",
      "Summary: 98 matched, 8 unmatched\n",
      "Low confidence matches (< 0.7): 13\n",
      "\n",
      "================================================================================\n",
      "Low confidence matches to review:\n",
      "  Score 0.67: Facial expression synthesis based on denoising dif... â†’ Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf\n",
      "  Score 0.57: Facial Expression Translation using Cycle Consiste... â†’ Cai et al. - 2021 - Identity-free facial expression recognition using conditional generative adversarial network.pdf\n",
      "  Score 0.69: Facial expression synthesis by u-net conditional g... â†’ Cai et al. - 2021 - Identity-free facial expression recognition using conditional generative adversarial network.pdf\n",
      "  Score 0.54: ExprADA: Adversarial domain adaptation for facial ... â†’ Kuang et al. - 2019 - Multi-expression generative adversarial networks for facial expression synthesis.pdf\n",
      "  Score 0.50: Conditional expression synthesis with face parsing... â†’ Zhai et al. - 2023 - Facial expression synthesis and recognition with pre-trained StyleGAN.pdf\n",
      "  Score 0.70: AnimateMe: 4D Facial Expressions viaÂ Diffusion Mod... â†’ Zou et al. - 2024 - 4D facial expression diffusion model.pdf\n",
      "  Score 0.56: Faceexpr: Personalized Facial Expression Generatio... â†’ Shu and Koike - 2025 - Pioneering facial expression generation from sEMG signals with diffusion models.pdf\n",
      "  Score 0.54: A highly naturalistic facial expression generation... â†’ Shu and Koike - 2025 - Pioneering facial expression generation from sEMG signals with diffusion models.pdf\n",
      "  Score 0.68: A facial expression synthesis method based on gene... â†’ Talking Face Generation with Expression-Tailored Generative Adversarial Network.pdf\n",
      "  Score 0.52: Facial Expression Analysis in Parkinsonsâ€™s Disease... â†’ Reviving Intentional Facial Expressions an Interface for ALS Patients using Brain Decoding and Imag.pdf\n",
      "âœ“ Match (score: 0.87): Semantic prior guided fine-grained facial expression manipul... â†’ Xue et al. - 2024 - Semantic prior guided fine-grained facial expression manipulation.pdf\n",
      "âœ“ Match (score: 0.80): Facial expression morphing: enhancing visual fidelity and pr... â†’ Sub-R-Pa et al. - 2024 - Facial expression morphing enhancing visual fidelity and preserving facial details in CycleGAN-base.pdf\n",
      "âœ“ Match (score: 0.93): Artificial intelligence D riven physical simulation and anim... â†’ Wang - 2025 - Artificial intelligence D riven physical simulation and animation generation in computer graphics.pdf\n",
      "âœ“ Match (score: 0.85): Facial expression generation from text with FaceCLIP... â†’ Fu et al. - 2025 - Facial expression generation from text with FaceCLIP.pdf\n",
      "\n",
      "================================================================================\n",
      "Summary: 98 matched, 8 unmatched\n",
      "Low confidence matches (< 0.7): 13\n",
      "\n",
      "================================================================================\n",
      "Low confidence matches to review:\n",
      "  Score 0.67: Facial expression synthesis based on denoising dif... â†’ Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf\n",
      "  Score 0.57: Facial Expression Translation using Cycle Consiste... â†’ Cai et al. - 2021 - Identity-free facial expression recognition using conditional generative adversarial network.pdf\n",
      "  Score 0.69: Facial expression synthesis by u-net conditional g... â†’ Cai et al. - 2021 - Identity-free facial expression recognition using conditional generative adversarial network.pdf\n",
      "  Score 0.54: ExprADA: Adversarial domain adaptation for facial ... â†’ Kuang et al. - 2019 - Multi-expression generative adversarial networks for facial expression synthesis.pdf\n",
      "  Score 0.50: Conditional expression synthesis with face parsing... â†’ Zhai et al. - 2023 - Facial expression synthesis and recognition with pre-trained StyleGAN.pdf\n",
      "  Score 0.70: AnimateMe: 4D Facial Expressions viaÂ Diffusion Mod... â†’ Zou et al. - 2024 - 4D facial expression diffusion model.pdf\n",
      "  Score 0.56: Faceexpr: Personalized Facial Expression Generatio... â†’ Shu and Koike - 2025 - Pioneering facial expression generation from sEMG signals with diffusion models.pdf\n",
      "  Score 0.54: A highly naturalistic facial expression generation... â†’ Shu and Koike - 2025 - Pioneering facial expression generation from sEMG signals with diffusion models.pdf\n",
      "  Score 0.68: A facial expression synthesis method based on gene... â†’ Talking Face Generation with Expression-Tailored Generative Adversarial Network.pdf\n",
      "  Score 0.52: Facial Expression Analysis in Parkinsonsâ€™s Disease... â†’ Reviving Intentional Facial Expressions an Interface for ALS Patients using Brain Decoding and Imag.pdf\n"
     ]
    }
   ],
   "source": [
    "matched_count = 0\n",
    "unmatched_count = 0\n",
    "low_confidence_matches = []\n",
    "titles = []\n",
    "pdf_names = []\n",
    "\n",
    "for metadata in zot.everything(zot.collection_items_top('SVUM4G2M', limit=106)):\n",
    "    if 'data' in metadata and 'title' in metadata['data']:\n",
    "        title = metadata['data']['title']\n",
    "\n",
    "        best_match, similarity = find_best_match(title, pdf_list, threshold=0.5)\n",
    "\n",
    "        if best_match:\n",
    "            matched_count += 1\n",
    "            print(f\"âœ“ Match (score: {similarity:.2f}): {title[:60]}... â†’ {best_match}\")\n",
    "            titles.append(title)\n",
    "            pdf_names.append(best_match)\n",
    "\n",
    "            if similarity < 0.7:\n",
    "                low_confidence_matches.append((title, best_match, similarity))\n",
    "        else:\n",
    "            unmatched_count += 1\n",
    "            print(f\"âœ— No match (best score: {similarity:.2f}): {title[:60]}...\")\n",
    "            titles.append(title)\n",
    "            pdf_names.append('')\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Summary: {matched_count} matched, {unmatched_count} unmatched\")\n",
    "print(f\"Low confidence matches (< 0.7): {len(low_confidence_matches)}\")\n",
    "\n",
    "if low_confidence_matches:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Low confidence matches to review:\")\n",
    "    for title, pdf, score in low_confidence_matches[:10]:\n",
    "        print(f\"  Score {score:.2f}: {title[:50]}... â†’ {pdf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a214c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'title': titles, 'pdf_name': pdf_names}).to_csv('zotero_pdf_matches.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0712a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "library = pd.read_csv('zotero_pdf_matches.csv', encoding='windows-1252')\n",
    "library.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bab4acad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 15:42:52,576 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-31 15:43:05,657 - INFO - HTTP Request: POST https://os-api.agno.com/telemetry/runs \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[36mâ”â”\u001b[0m\u001b[36m Message \u001b[0m\u001b[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[36mâ”â”“\u001b[0m\n",
      "\u001b[36mâ”ƒ\u001b[0m                                                                              \u001b[36mâ”ƒ\u001b[0m\n",
      "\u001b[36mâ”ƒ\u001b[0m \u001b[32mShare a 2 sentence horror story.\u001b[0m                                             \u001b[36mâ”ƒ\u001b[0m\n",
      "\u001b[36mâ”ƒ\u001b[0m                                                                              \u001b[36mâ”ƒ\u001b[0m\n",
      "\u001b[36mâ”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›\u001b[0m\n",
      "\u001b[34mâ”â”\u001b[0m\u001b[34m Response (13.8s) \u001b[0m\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[34mâ”â”“\u001b[0m\n",
      "\u001b[34mâ”ƒ\u001b[0m                                                                              \u001b[34mâ”ƒ\u001b[0m\n",
      "\u001b[34mâ”ƒ\u001b[0m \u001b[3mI woke to my phone lighting up with a photo of me sleeping, taken from \u001b[0m      \u001b[34mâ”ƒ\u001b[0m\n",
      "\u001b[34mâ”ƒ\u001b[0m \u001b[3minside my dark bedroom. I live alone.\u001b[0m                                        \u001b[34mâ”ƒ\u001b[0m\n",
      "\u001b[34mâ”ƒ\u001b[0m                                                                              \u001b[34mâ”ƒ\u001b[0m\n",
      "\u001b[34mâ”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "from os import getenv\n",
    "from agno.agent import Agent\n",
    "from agno.models.openrouter import OpenRouter\n",
    "\n",
    "agent = Agent(\n",
    "    model=OpenRouter(id=\"gpt-5-mini\", api_key=\"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\"),\n",
    "    markdown=True\n",
    ")\n",
    "\n",
    "# Print the response in the terminal\n",
    "agent.print_response(\"Share a 2 sentence horror story.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
